# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.5.2
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# + [markdown] slideshow={"slide_type": "slide"}
# # Необходимые сведения из линейной алгебры &mdash; 1 #
# **
# -

import numpy as np
from numpy import linalg as LA

# ## Введение ##
#
# Что мы хотим вспомнить из линейной алгебры? Мы хотим уметь решать пять базовых задач:
#
# 1. Найти $x$ в уравнении $Ax = b$;
# 1. Найти $x$ и $\lambda$ в уравнении $Ax = \lambda x$;
# 1. Найти $v$, $u$ и $\sigma$ в уравнении $Av = \sigma u$;
# 1. Найти минимум выражения $\dfrac{\lVert Ax \rVert^2}{\lVert x \rVert^2}$;
# 1. Найти разложение матрицы $A$.
#
#
# 1. Можно ли вектор $b$ представить в виде линейной комбинации векторов матрицы $A$?
# 1. Вектор $Ax$ имеет то же направление, что и вектор $x$. Вдоль этого направления все сложные взаимодействия с матрицей $A$ чрезвычайно упрощаются. Вектор $A^2 x$ становится просто $\lambda^2 x$, матрица $e^{At}$ &mdash; $e^{\lambda t}$. Все действия становятся линейными.
# 1. Уравнение $Av = \sigma u$ похоже на предыдущее. Но матрица $A$ больше не квадратная. Это сингулярное разложение
# 1. Минимизация и факторизация являются фундаментальными прикладными задачами.

# ---

# + [markdown] slideshow={"slide_type": "slide"}
# ## Умножение матриц ##
#
# Давайте начнём с умножения матриц.
#
# ### Умножение матрицы на вектор ###
#
# Для начала введём обозначения. Вектор будем записывать в виде столбца и обозначать вектором $\vec{a}$ или жирной буквой $\mathbf{a}$.
# С помощью $\vec{a}^\top$ или $\mathbf{a}^\top$ будем обозначать транспонировнный столбец, т.е. строку:
# $$
#   \mathbf{a} =
#   \begin{pmatrix}
#      a_1    \\
#      \cdots \\
#      a_n    \\
#   \end{pmatrix},
#   \mathbf{a}^\top = (a_1, \ldots, a_n).
# $$
#
# Рассмотрим умножение матрицы $A$ размером $l \times n$ на вектор: $A \mathbf{x}$.
#
# По определению произведение $A \mathbf{x}$ есть вектор, в котором на $i$-ом месте находится скалярное произведение $i$-ой *строки* на столбец $\mathbf{x}$:
# $$
#   A \mathbf{x} = 
#   \begin{pmatrix}
#     -\, \mathbf{a}_1^\top \,- \\
#     \cdots \\
#     -\, \mathbf{a}_i^\top \,- \\
#     \cdots \\
#     -\, \mathbf{a}_m^\top \,- \\
#   \end{pmatrix}
#   \cdot \mathbf{x} = 
#   \begin{pmatrix}
#     \mathbf{a}_1^\top \cdot \mathbf{x} \\
#     \cdots \\
#     \mathbf{a}_i^\top \cdot \mathbf{x} \\
#     \cdots \\
#     \mathbf{a}_m^\top \cdot \mathbf{x} \\
#   \end{pmatrix}.
# $$
#
# Но можно посмотреть на это иначе, как на произведение *столбцов* матрицы $A$ на элементы вектора $\mathbf{x}$:
# $$
#   A \mathbf{x} = 
#   \begin{pmatrix}
#      | & {} & | & {} & | \\
#      \mathbf{a}_1 & \cdots & \mathbf{a}_i & \cdots & \mathbf{a}_n \\
#      | & {} & | & {} & | \\
#   \end{pmatrix}
#   \begin{pmatrix}
#      x_1    \\
#      \cdots \\
#      x_i    \\
#      \cdots \\
#      x_m \\
#   \end{pmatrix}
#   = 
#   x_1 \mathbf{a}_1 + \dots + x_i \mathbf{a}_i + \dots + x_m \mathbf{a}_m.
# $$
# Таким образом, результирующий вектор есть *линейная комбинация* столбцов матрицы $A$ с коэффициентами из вектора $\mathbf{x}$.
# -

# ### Линейная оболочка, размерность пространства, ранг матрицы ###
#
# Посмтрим ещё раз на задачу № 1 из списка выше: $A \mathbf{x} = \mathbf{b}$.
# Теперь её можно переформулировать так: ответить, можно ли столбец $\mathbf{b}$ представить в виде линейной комбинации столбцов матрицы $A$.
#
# Поясним на примере.
# Пусть
# $$
#   A = 
#   \begin{pmatrix}
#      1 & 2 & 3 \\
#      2 & 1 & 3 \\
#      3 & 1 & 4 \\
#   \end{pmatrix}
# $$
# Что мы можем сказать о линейной оболочке её столбцов? Что это за пространство? Какой размерности?
#
# **Определение 1.** Рангом матрицы $A$ с $n$ строк и $m$ столбцов называется максимальное число линейно независимых столбцов (строк).
#
# **Определение 2.** *Скелетным разложением* матрицы $A$ размеров $m \times n$ и ранга $r>0$ называется разложение вида $A = CR$, где матрицы $C$ и $R$ имеют размеры соттвественно $m \times r$ и $r \times n$.
#
# Так как ранг произведения не превосходит рангов сомножителей, а ранги $C$ и $R$ не могут быть больше $r$, то они равны $r$.
#
# $$
#   A = 
#   \begin{pmatrix}
#      1 & 2 & 3 \\
#      2 & 1 & 3 \\
#      3 & 1 & 4 \\
#   \end{pmatrix}
#   =
#   \begin{pmatrix}
#      1 & 2 \\
#      2 & 1 \\
#      3 & 1 \\
#   \end{pmatrix}
#   \cdot
#   \begin{pmatrix}
#      1 & 0 & 1 \\
#      0 & 1 & 1 \\
#   \end{pmatrix}
#   = C \cdot R.
# $$
#
# $C$ &mdash; базисные столбцы, $R$ &mdash; базисные строки.
# Это разложение доказывает теорему: ранг матрицы по столбцам (количество независимых столбцов) равен рангу матрицы по строкам (количество независимых строк).

# Существует другой вариант скелетного разложения: $A = CMR$.
# В этом случае матрица $С$, как и ранее, состоит из $r$ независимых столбцов матрицы $A$, матрица $R$ теперь состоит из $r$ независимых строк матрицы $A$, а матрица $M$ размером $r \times r$ называется смешанной матрицей (mixing matrix).
# Для $M$ можно получить следующую формулу:
# $$
#   A = CMR \\
#   C^\top A R^\top = C^\top C M R R^\top \\
#   M = (C^\top C)^{-1} (C^\top A R^\top) (R R^\top)^{-1}.
# $$
#
# Обратим внимание на используемый приём &mdash; домножение прямоугольной матрицы на транспонированную. Полученную квадратную матрицу (в случае если она невырожденная) можно уже домножать на обратную (&laquo;делить на матрицу&raquo;).

# ***Разложение матрицы на сумму матриц ранга 1.***
#
# Применяем умножение &laquo;столбец на строку&raquo;:
# $$
#   A = 
#   \begin{pmatrix}
#      1 & 2 & 3 \\
#      2 & 1 & 3 \\
#      3 & 1 & 4 \\
#   \end{pmatrix}
#   =
#   \begin{pmatrix}
#      1 & 2 \\
#      2 & 1 \\
#      3 & 1 \\
#   \end{pmatrix}
#   \cdot
#   \begin{pmatrix}
#      1 & 0 & 1 \\
#      0 & 1 & 1 \\
#   \end{pmatrix}
#   =
#   \begin{pmatrix}
#      1 & 0 & 1 \\
#      2 & 0 & 2 \\
#      3 & 0 & 3 \\
#   \end{pmatrix}
#   +
#   \begin{pmatrix}
#      0 & 2 & 2 \\
#      0 & 1 & 1 \\
#      0 & 1 & 1 \\
#   \end{pmatrix}.
# $$

# ---

# ## Замечательные матрицы ##
#
# 1. Симметричная матрица
# 1. Симметричная положительно определённая матрица
# 1. Ортогональная матрица
#
# **Примеры**: $A^\top A$, матрица ортогональной проекции, матрицы поворота и симметрии.

# ### Матрица ортогональной проекции ###
#
# Пусть заданы вектор $\mathbf{b}$ из $\mathbb{R}_m$ и набор векторов $\mathbf{a}_i$, образующих базис в $\mathbb{R}_n$ ($n<m$).
#
# Требуется найти ортогональную проекцию вектора $\mathbf{b}$ на линейную оболочку векторов $\mathbf{a}_i$.
#
# Искомый вектор является линейной комбинацией базисных векторов $\mathbf{a}_i$ с неизвестными коэффициентами $x_i$.
# Запишем это в виде произведения матрицы $A$, столбцы которой являются векторами $\mathbf{a}_i$, на неизвестный вектор $\mathbf{x}$: $A\mathbf{x}$.
# Мы ищем ортогональную проекцию на пространство столбцов матрицы $A$, поэтому вектор $A\mathbf{x} - \mathbf{b}$ должен быть ортогонален *любому* вектору $A\mathbf{y}$.
# Запишем это через скалярное произведение:
# $$
#   (A \mathbf{y})^\top(A\mathbf{x} - \mathbf{b}) = \mathbf{y}^\top (A^\top A \mathbf{x} - A^\top \mathbf{b}) = 0.
# $$
#
# Это српаведливо для произвольного вектора $\mathbf{y}$, откуда следует, что
# $$
#   A^\top A \mathbf{x} = A^\top \mathbf{b}.
# $$
#
# Ранг матрицы $A^\top A$ равен рангу $A$, а столбцы матрицы $A$ линейно независимы, следовательно матрица $A^\top A$ обратима.
# Отсюда находим выражение для вектора коэффициентов проекции и сам вектор проекции $\mathbf{p} = A\mathbf{x}$
# $$
#   \mathbf{x} = (A^\top A)^{-1} A^\top \mathbf{b}, \
#   \mathbf{p} = A (A^\top A)^{-1} A^\top \mathbf{b}.
# $$
#
# Матрица $P = A (A^\top A)^{-1} A^\top \mathbf{b}$, осуществляющая проекцию, называется *матрицей ортогонального проектирования*.
# Матрица ортопроектирования обладает двумя основными свойствами:
#
# 1. $P^2 = P$ &mdash; характеристическое свойство всех проекторов,
# 1. $P^\top = P$ &mdash; отличительное свойство ортогонального проектора.

# ---

# ## Разложения матриц##
#
# Рассмотрим пять замечательных разложений матриц
#
# 1. $A = LU$
# 1. $A = QR$
# 1. $A = X \Lambda X^{-1}$
# 1. $S = Q \Lambda Q^\top$
# 1. $A = U \Sigma V^\top$

# ### 1. $\mathbf{LU}$-разложение ###
#
# **TODO** Связь с Гауссом
#
# **Определение.** $LU$-разложением матрицы $A$ называется разложение матрицы $A$ в произведение $LU$ невырожденной нижней треугольной матрицы $L$ и верхней треугольной матрицы $U$ с единицами на главной диагонали.
#
# **Теорема (критерий существования).** $LU$-разложение матрицы $A$ существует тогда и только тогда, когда все главные миноры матрицы $A$ отличны от нуля. Если $LU$-разложение существует, то оно единственно.
#
# **Замечание.** Единственным является разложение на такие треугольные множители, что у второго из них на главной диагонали стоят единицы. Вообще же существует много треугольних разложений, в частности такое, в котором единицы находятся на главной диагонли у первого сомножителя.
#
# **Замечание.** $LU$-разложение можно рассматривать, как матричную форму записи метода исключения Гаусса.
#
# Матрицу $L$ можно представить как произведение матрицы $L_1$, имеющей единицы на главной диагонли, и диагональной матрицы $D$. Тогда мы получим
# **Предложение.** Матрицу $A$, главные миноры которой не равны нулю, можно единственным образом разложить в произведение $L_1 D U$, в котором $D$ &mdash; диагональная, а $L_1$ и $U$ &mdash; нижняя и верхняя треугольные матрицы с единицами на главной диагонали.
#
# Рассмотрим важный частный случай &mdash; $LDU$-разложение симметричной матрицы $S$.
# Тогда $S = U^\top D L^\top$, причём $U^\top$ &mdash; нижняя, а $L^\top$ &mdash; верхняя треугольны матрицы. В силу единственности разложениея получаем
# $$
#   S = U^\top D U.
# $$
#
# Если к тому же $S$ положительно определена, то все диагональные элементы матрицы $D$ положительны и мы можем ввести матрицы $D^{1/2} = \mathrm{diag}\left(\sqrt{d_1}, \dots, \sqrt{d_n}\right)$ и $V = D^{1/2}U$. Тогда мы получаем *разложение Холецкого*
# $$
#   S = V^\top V.
# $$
# Разложение Холецкого играет заметную роль в численных методах, так как существует эффективный алгоритм, позволяющий получить его для положительно определённой симметричной матрицы $S$.

# ### 2. $\mathbf{QR}$-разложение ###
#
# Большую роль в численных методах играет разложение *квадратной матрицы* $A$ на ортогональную матрицу $Q$ и верхнетреугольную матрицу $R$.
# $$
#   A = QR.
# $$
#
# **Предложение 1.** Любая квадратная матрица $A$ может быть разложена в произведение $QR$. \
# **Предложение 2.** Если матрица $A$ невырождена, то её $QR$-разложение, в котором диагональные элементы $R$ положительны, единственно. \
# **Предложение 3.** $QR$-разложение тесно связано с процессом ортогонализации Грама &mdash; Шмидта. Действительно, $AU = Q$ равносильно $QR$-разложению с $R = U^{-1}$.
#
#
#
#
# **Пример.** Рассмотрим систему линейных уравнений $A\mathbf{x} = \mathbf{b}$. Используя $QR$-разложение матрицы $A$, получим
#
# $$
#   QR \mathbf{x} = \mathbf{b}, \\
#   R \mathbf{x} = Q^\top \mathbf{b}.
# $$
#
# И в случе невырожденной $R$
# $$
#   \mathbf{x} = R^{-1}Q^\top \mathbf{b}.
# $$

# ### 2*. $\mathbf{QR}$-разложение для прямоугольных матриц ###
#
# Произвольную матрицу размером $m \times n$ можно представить в виде разложения
# $$
#   A = QR,
# $$
# где $Q$ &mdash; ортогональная матрица порядка $m$, а $R$ &mdash; матрица размеров $m \times n$, элементы $r_{ij}$ которой удовлетворяют условию $r_{ij}=0$ при $i>j$.
# Такое разложение назовём $Qr$-разложением, чтобы подчеркнуть, что матрица $R$, вообще говоря, не является квадратной.
#
# Второе обобщение $QR$-разложения можно получить, применяя метод ортогонализации Грама &mdash; Шмидта.
# Теперь матрица $Q$ размеров $m \times n$ может рассматриваться как совокупность $n$ столбцов из некоторой ортогональной матрицы порядка $m$, а $R$ &mdash; квадратная верхняя треугольная матрица порядка $n$.
# Такое разложение будем называть $qR$-разложением.







# +
np.random.seed(12345)

D = np.diag((15, 10, 5))
B = np.random.randint(0, 10, (3,3))
# print(B)
A = B @ D @ LA.inv(B)
print(LA.eigvals(A))

A1 = A
for i in range(10):
    Q, R = LA.qr(A1)
    A1 = R@Q
    print(np.diag(A1))
# -









import numpy as np
from numpy import linalg as LA

A = np.array([[1,2,3],[2,1,3],[3,1,4]])
C = np.array([[1,2],[2,1],[3,1]])
R = np.array([[1,2,3],[2,1,3]])

M = LA.inv(C.T@C) @ (C.T@A@R.T) @ LA.inv(R@R.T)
print(M)
B = C@M@R
print(B)

# + [markdown] slideshow={"slide_type": "skip"}
# ---

# + [markdown] slideshow={"slide_type": "slide"}
# ## Литература ##
#
# 1. G. Strang. Linear algebra and learning from data. Wellesley-Cambridge Press, 2019. 432 p.
# 1. Д.В. Беклемишев. Дополнительные главы линейной алгебры. М.: Наука, 1983. 336 с.
# -


