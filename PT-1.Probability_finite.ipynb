{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Необходимые сведения из теории вероятностей &mdash; 1 #\n",
    "*Элементарная теория вероятности*\n",
    "\n",
    "> Мы называем элементарной теорией вероятностей ту часть теории вероятностей, в которой приходится иметь дело с вероятностями лишь конечного числа событий. \\\n",
    "А. Н. Колмогоров. &laquo;Основные понятия теории вероятностей&raquo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Предмет теории вероятностей ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Предметом теории вероятностей является математический анализ случайных явлений &mdash; эмпирических феноменов, которые (при заданном «комплексе условий») могут быть охарактеризованы тем, что\n",
    "\n",
    "- для них отсутствует *детерминистическая регулярность* (наблюдения над ними не всегда приводят к одним и тем же исходам)\n",
    "\n",
    "и в то же самое время\n",
    "\n",
    "- они обладают некоторой *статистической регулярностью* (проявляющейся в статистической устойчивости частот)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Поясним сказанное на классическом примере «честного» подбрасывания «правильной» монеты.\n",
    "Ясно, что заранее невозможно с определенностью предсказать исход каждого подбрасывания.\n",
    "Результаты отдельных экспериментов носят крайне нерегулярный характер (то «герб», то «решетка»), и кажется, что это лишает нас возможности познать какие-либо закономерности, связанные с этими экспериментами.\n",
    "Однако, если провести большое число «независимых» подбрасываний, то можно заметить, что для «правильной» монеты будет наблюдаться вполне определенная статистической регулярность, проявляющаяся в том, что частота выпадания «герба» будет «близка» к $1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Вероятностная модель ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Множество исходов ###\n",
    "\n",
    "Рассмотрим некоторый эксперимент, результаты которого описываются конечным числом различных *исходов* $\\omega_1, \\dots , \\omega_N$. Для нас несущественна реальная природа этих исходов, важно лишь то, что их число $N$ конечно.\n",
    "\n",
    "Исходы $\\omega_1, \\dots , \\omega_N$ будем также называть *элементарными событиями*, а их совокупность\n",
    "\n",
    "$$ \\Omega = \\{ \\omega_1, \\dots , \\omega_N \\} $$\n",
    "\n",
    "*пространством элементарных событий* или *пространством исходов*.\n",
    "\n",
    "Выделение пространства элементарных событий представляет собой первый шаг в формулировании понятия *вероятностной модели* (вероятностной &laquo;теории&raquo;) того или иного эксперимента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    ">**Примеры**\n",
    ">\n",
    "> - однократное подбрасывание монеты (пространство исходов состоит из двух точек: Г — «герб», Р — «решетка»);\n",
    "> - n-кратное подбрасывание монеты;\n",
    "> - выбор шаров с возвращением (упорядоченные и неупорядоченные выборки);\n",
    "> - выбор шаров без возвращения (упорядоченные и неупорядоченные выборки)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Алгебра ###\n",
    "\n",
    "Наряду с понятием пространства элементарных событий введём теперь важное понятие события, лежащее в основе построения всякой\n",
    "вероятностной модели («теории») рассматриваемого эксперимента.\n",
    "\n",
    "Мы собираемся определить набор подмножеств множества $\\Omega$, которые будут называться событиями, и затем задать вероятность как функцию, определённую *только* на множестве событий.\n",
    "\n",
    "Итак, событиями мы будем называть не любые подмножества $\\Omega$, а лишь элементы некоторого выделенного набора подмножеств множества $\\Omega$.\n",
    "При этом необходимо позаботиться, чтобы этот набор подмножеств был замкнут относительно обычных операций над событиями, т. е. чтобы объединение, пересечение и дополнение событий снова давало событие.\n",
    "Введём понятие алгебры множеств.\n",
    "\n",
    "**Определение.** Множество $\\mathcal{A}$, элементами которого являются подмножества множества $\\Omega$ называется *алгеброй*, если оно удовлетворяет следующим условиям:\n",
    "\n",
    "1. $\\Omega \\in \\mathcal{A}$ (алгебра содержит достоверное событие);\n",
    "2. если $A \\in \\mathcal{A}$, то $\\overline{A} \\in \\mathcal{A}$ (вместе с любым множеством алгебра содержит противоположное к нему);\n",
    "3. если $A \\in \\mathcal{A}$ и $B \\in \\mathcal{A}$, то $A \\cup B \\in \\mathcal{A}$ (вместе с любыми двумя множествами алгебра содержит их объединение).\n",
    "\n",
    "Из условий 1 и 2 следует, что пустое множество $\\emptyset = \\overline{\\Omega}$ также содержится в $\\mathcal{A}$, т. е. алгебра содержит и невозможное событие.\n",
    "\n",
    "В качестве систем событий целесообразно рассматривать такие системы множеств, которые являются алгебрами. Именно такие системы событий мы и будем рассматривать далее.\n",
    "\n",
    "Остановимся на некоторых примерах алгебр событий:\n",
    "\n",
    "1. $\\mathcal{A} = \\{ \\Omega, \\emptyset \\}$ &mdash; система, состоящая из $\\Omega$ и пустого множества (так называемая тривиальная алгебра);\n",
    "2. $\\mathcal{A} = \\{ A, \\overline{A}, \\Omega, \\emptyset \\}$ &mdash; система, порожденная событием $A$;\n",
    "3. $\\mathcal{A} = \\{ A: A \\subseteq \\Omega \\}$ &mdash; совокупность всех (включая и пустое множество $\\emptyset$) подмножеств $\\Omega$.\n",
    "\n",
    ">**Упражнение.** Доказать, что если $\\Omega$ состоит из $n$ элементов, то в множестве всех его подмножеств ровно $2^n$ элементов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Вероятностная мера ###\n",
    "\n",
    "Пока мы сделали два первых шага к построению вероятностной модели эксперимента с конечным числом исходов: выделили пространство исходов $\\Omega$ и некоторую систему $\\mathcal{A}$ его подмножеств, образующих алгебру и называемых событиями.\n",
    "Сделаем теперь следующий шаг, а именно припишем каждому элементарному событию (исходу, явлению) $\\omega_i \\in \\Omega$, $i=1, \\ldots, N$ некоторый &laquo;вес&raquo;, обозначаемый $p(\\omega_i)$ (или $p_i$) и называемый *вероятностью* исхода $\\omega_i$, который будем считать удовлетворяющим следующим условиям:\n",
    "\n",
    "1. $0  \\le p(\\omega_i) \\le 1$ (*неотрицательность*),\n",
    "2. $p(\\omega_1) + \\ldots + p(\\omega_N) = 1$ (*нормированность*).\n",
    "\n",
    "Отправляясь от заданных вероятностей $p(\\omega_i)$ исходов $\\omega_i$ определим *вероятность* $\\mathrm{P}(A)$ любого события $A \\in \\mathcal{A}$ по формуле\n",
    "$$ \\mathrm{P}(A) = \\sum_{\\{i:\\omega_i \\in A\\}} p(\\omega_i). $$\n",
    "\n",
    "#### Свойства вероятностей ####\n",
    "\n",
    "1. $\\mathrm{P}(\\emptyset) = 0$,\n",
    "1. $\\mathrm{P}(\\Omega) = 1$,\n",
    "1. $\\mathrm{P}(A \\cup B) = \\mathrm{P}(A) + \\mathrm{P}(B) - \\mathrm{P}(A \\cap B)$,\n",
    "1. $\\mathrm{P}(\\overline{A}) = 1 - P(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вероятностное пространство ###\n",
    "\n",
    "**Определение.** Принято говорить, что &laquo;вероятностное пространство&raquo;\n",
    "\n",
    "$$ \\left( \\Omega, \\mathcal{A}, \\mathrm{P} \\right), $$\n",
    "\n",
    "где $\\Omega = {\\omega_1, \\ldots, \\omega_N}$, $\\mathcal{A}$ — некоторая алгебра подмножеств $\\Omega$ и $\\mathrm{P} = \\{ \\mathrm{P}(A); A \\in \\mathcal{A} \\}$,\n",
    "определяет вероятностную модель эксперимента с пространством исходов (элементарных событий) $\\Omega$ и алгеброй событий $\\mathcal{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Замечания ###\n",
    "\n",
    "При построении вероятностных моделей в конкретных ситуациях выделение пространства элементарных событий $\\Omega$ и алгебры событий $\\mathcal{A}$, как правило, не является сложной задачей. При этом в элементарной теории вероятностей в качестве алгебры $\\mathcal{A}$ обычно берется алгебра *всех* подмножеств $\\Omega$. Труднее обстоит дело с вопросом о том, как задавать вероятности элементарных событий. В сущности, ответ на этот вопрос лежит вне рамок теории вероятностей, и мы его подробно не рассматриваем, считая, что основной нашей задачей является не вопрос о том, как приписывать исходам те или иные вероятности, а *вычисление* вероятностей\n",
    "сложных событий (событий из $\\mathcal{A}$) по вероятностям элементарных событий.\n",
    "\n",
    "С математической точки зрения ясно, что в случае конечного пространства элементарных событий с помощью приписывания исходам $\\omega_1, \\ldots , \\omega_N$ неотрицательных чисел $p_1, \\ldots , p_N$, удовлетворяющих условию $p_1 + \\ldots + p_N = 1$, мы получаем все мыслимые (конечные) вероятностные пространства.\n",
    "\n",
    "*Правильность* же назначенных для конкретной ситуации значений $p_1, \\ldots , p_N$ может быть до известной степени проверена с помощью *закона больших чисел*, согласно которому в длинных сериях &laquo;независимых&raquo; экспериментов, происходящих при одинаковых условиях, частоты появления элементарных событий &laquo;близки&raquo; к их вероятностям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Некоторые классические модели и распределения ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Биномиальное распределение ###\n",
    "\n",
    "Предположим, что монета подбрасывается $n$ раз и результат наблюдений записывается в виде упорядоченного\n",
    "набора $(a_1, \\ldots, a_n)$, где $a_i = 1$ в случае появления «герба» («успех») и $a_i = 0$ в случае появления «решетки» («неуспех»).\n",
    "Пространство всех исходов имеет следующую структуру:\n",
    "$$ \\Omega= \\left\\{ \\omega: \\omega = (a_1, \\ldots, a_n), \\quad a_i = 0 \\; \\mathrm{или} \\; 1 \\right\\}. $$\n",
    "\n",
    "Припишем каждому элементарному событию $\\omega = (a_1, \\ldots, a_n)$ вероятность (&laquo;вес&raquo;)\n",
    "$$ p(\\omega) = p^{\\sum a_i} q^{n-\\sum a_i}, $$\n",
    "где неотрицательные числа $p$ и $q$ таковы, что $p + q = 1$.\n",
    "\n",
    "Итак, пространство $\\Omega$ вместе с системой $\\mathcal{A}$ всех его подмножеств и вероятностями $\\mathrm{P}(A) = \\sum\\limits_{\\omega \\in A}p(\\omega), \\; A \\in \\mathcal{A}$ (в частности, $\\mathrm{P}(\\{\\omega\\}) = p(\\omega), \\; \\omega \\in \\Omega$) определяет некоторую вероятностную модель.\n",
    "Естественно назвать её *вероятностной моделью, описывающей $n$-кратное подбрасывание монеты*.\n",
    "\n",
    "Введём в рассмотрение события\n",
    "$$ \n",
    "    A_k = \\left\\{\\omega: \\omega=(a_1, \\ldots, a_n), a_1 + \\ldots + a_n = k\\right\\}, \\quad k = 0, 1, \\ldots, n,\n",
    "$$\n",
    "означающие, что произойдет в точности $k$ &laquo;успехов&raquo;. Тогда вероятность события $A_k$ равна\n",
    "$$ \\mathrm{P}(A_k) = C_n^k p^k q^{n-k}, $$\n",
    "причём $\\sum\\limits_{k=0}^n \\mathrm{P}(A_k) = 1$.\n",
    "\n",
    "Набор вероятностей $\\{\\mathrm{P}(A_0), \\ldots,\\mathrm{P}(A_n)\\}$ называется *биномиальным распределением* (числа &laquo;успехов&raquo; в выборке объёма $n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Гипергеометрическое распределение ###\n",
    "\n",
    "Рассмотрим урну, содержащую $N$ шаров, из которых $M$ шаров имеют белый цвет.\n",
    "Предположим, что осуществляется выбор без возвращения объёма $n < N$.\n",
    "Вероятность события $B_m$, состоящего в том, что $m$ шаров из выборки имеют белый цвет равна\n",
    "\n",
    "$$ \\mathrm{P}(B_m) = \\dfrac{C_M^m C_{N-M}^{n-m}}{C_N^n}. $$\n",
    "\n",
    "Набор вероятностей $\\{\\mathrm{P}(B_0), \\ldots,\\mathrm{P}(B_n)\\}$ носит название многомерного гипергеометрического распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Оценка максимального правдоподобия ##\n",
    "*Задача об оценке генеральной совокупности по выборке*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Пусть $N$ &mdash; размер некоторой популяции, который требуется оценить &laquo;минимальными средствами&raquo; без простого пересчета всех элементов этой совокупности.\n",
    "Подобного рода вопрос интересен, например, при оценке числа жителей в той или иной стране, городе и т. д.\n",
    "\n",
    "В 1786 г. Лаплас для оценки числа $N$ жителей Франции предложил следующий метод.\n",
    "Выберем некоторое число, скажем, $M$, элементов популяции и пометим их. Затем возвратим их в основную совокупность и предположим, что они &laquo;хорошо перемешаны&raquo; с немаркированными элементами.\n",
    "После этого возьмем из &laquo;перемешанной&raquo; популяции $n$ элементов.\n",
    "Пусть среди них $m$ элементов оказались маркированными.\n",
    "\n",
    "Cоответствующая вероятность $\\mathrm{P}(B_m(N))$ задается формулой гипергеометрического распределения:\n",
    "$$\n",
    "    \\mathrm{P}(B_m(N)) = \\frac{C_M^m C_{N-M}^{n-m}}{C_N^n}. \\tag{1}\\label{eq:prob}\n",
    "$$\n",
    "\n",
    "Нам известны числа $M$, $n$ и $m$, а $N$ (число жителей Франции) &mdash; нет, его требуется оценить.\n",
    "Пусть, например, $M=1000$, $n=1000$, а $m=10$.\n",
    "Тогда всё, что нам достоверно известно о населении Франции, это $N \\ge n + M - m = 1990$.\n",
    "Вообще говоря, не исключено, что во Франции и живёт ровно $1990$ человек.\n",
    "Однако, отправляясь от этой гипотезы, мы придём к выводу, что случилось событие фантастически малой вероятности.\n",
    "Действительно, вероятность того, что выборка объёмом $n=1000$ из генеральной совокупности объёма $N=1990$ будет содержать $m=10$ маркированных объектов, если общее число маркированных объектов $M=1000$, по формуле $\\eqref{eq:prob}$ равна\n",
    "$$\n",
    "    \\mathrm{P}(B_{10}(1990)) = \\frac{C_{1000}^{10} C_{990}^{990}}{C_{1990}^{1000}} = \\frac{(1000!)^2}{10! \\, 1990!} \\sim 10^{-575}.\n",
    "$$\n",
    "\n",
    "Аналогичное рассуждение заставляет нас откинуть гипотезу о том, что $N$ очень велико, скажем равно миллиону ($\\mathrm{P}(B_{10}(10^6)) \\sim 10^{-7}$).\n",
    "Такие рассуждения приводят нас к отысканию такого $N$, при котором $\\mathrm{P}(B_m(N))$ является наибольшим, так как при этом значении $N$ наблюдённый результат имеет наибольшую вероятность.\n",
    "Для каждого частного набора наблюдений $M$, $n$ и $m$ значение $N$, при котором $\\mathrm{P}(B_m(N))$ максимально, обозначается через $\\hat{N}$ и называется оценкой *максимального правдоподобия*.\n",
    "\n",
    "Можно показать, что наиболее правдоподобное $\\hat{N}$ определяется формулой ($[\\cdot]$ &mdash; целая часть):\n",
    "$$ \\hat{N} = \\left[\\dfrac{Mn}{m}\\right]. \\tag{2}\\label{eq:max} $$\n",
    "\n",
    "В нашем примере оценкой максимального правдоподобия для число жителей Франции является число $\\hat{N} = 10^5$ ($\\mathrm{P}(B_{10}(10^5)) \\approx 0.126$).\n",
    "\n",
    ">*Задание.* Получить формулу $\\eqref{eq:max}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Условная вероятность. Независимость ##\n",
    "\n",
    "### Понятие условной вероятности ###\n",
    "\n",
    "Понятие *вероятности* события дает нам возможность ответить на вопрос такого типа: если урна содержит $M$ шаров, из которых $M_1$ шаров белого цвета и $M_2$ &mdash; чёрного, то какова вероятность $\\mathrm{P}(A)$ события $A$, состоящего в том, что вытащенный шар имеет белый цвет?  \n",
    "В случае классического подхода $\\mathrm{P}(A) =M_1/M$.\n",
    "\n",
    "Вводимое ниже понятие условной вероятности позволяет отвечать на вопрос следующего типа: какова вероятность того, что второй извлеченный шар белого цвета (событие $B$), при условии, что первый шар также имеет белый цвет (событие $A$)? Рассматривается выбор без возвращения.\n",
    "\n",
    "Естественно здесь рассуждать так: если первый извлеченный шар имел белый цвет, то перед вторым извлечением мы имеем урну с $M-1$ шаром, из которых $M_1 - 1$ шаров имеют белый цвет, а $M_2$ &mdash; чёрный; поэтому интуитивно представляется целесообразным считать, что интересующая нас (условная) вероятность равна $\\dfrac{M_1-1}{M-1}$.\n",
    "\n",
    "Дадим теперь определение условной вероятности, согласующееся с интуитивными представлениями о ней.\n",
    "\n",
    "**Определение.** Условной вероятностью события $B$ при условии события $A$ с $\\mathrm{P}(A)>0$ (обозначение: $\\mathrm{P}(B|A)$) называется величина\n",
    "$$ \\dfrac{\\mathrm{P}(AB)}{\\mathrm{P}(A)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Свойства условной вероятности ###\n",
    "\n",
    "1. $\\mathrm{P}(A|A) = 1$,\n",
    "1. $\\mathrm{P}(\\emptyset|A) = 0$,\n",
    "1. $\\mathrm{P}(B|A) = 1$, $B \\supseteq A$,\n",
    "1. $\\mathrm{P}(B_1 + B_2|A) = \\mathrm{P}(B_1|A) + \\mathrm{P}(B_2|A)$.\n",
    "\n",
    "> **Пример.** Рассмотрим семьи, имеющие двух детей. Спрашивается, какова вероятность того, что в семье оба ребенка мальчики, в предположении, что:\n",
    "1. старший ребенок &mdash; мальчик;\n",
    "2. по крайней мере один из детей &mdash; мальчик?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Формула полной вероятности ###\n",
    "\n",
    "Рассмотрим *полную группу несовмстимых событий* $\\mathcal{D} = \\{A_1, \\dots, A_n\\}$. Имеет место **формула полной вероятности**\n",
    "$$ \\mathrm{P}(B) = \\sum_{i=1}^n \\mathrm{P}(B|A_i) \\mathrm{P}(A_i). $$\n",
    "\n",
    "> **Пример.** В урне имеется $M$ шаров, среди которых $m$ &laquo;счастливых&raquo;. Спрашивается, какова вероятность извлечь на втором шаге &laquo;счастливый&raquo; шар (предполагается, что качество первого извлеченного шара неизвестно).\n",
    "\n",
    "Справедлива **формула умножения вероятностей**:\n",
    "$$ \\mathrm{P}(AB) = \\mathrm{P}(B|A) \\mathrm{P}(A). $$\n",
    "\n",
    "По индукции:\n",
    "$$ \\mathrm{P}(A_1, \\dots, A_n) = \\mathrm{P}(A_1) \\mathrm{P}(A_2|A_1) \\dots \\mathrm{P}(A_n|A_1 \\dots A_{n-1}). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Независимость ###\n",
    "\n",
    "**Определение.** События $A$ и $B$ называются *независимыми* или *статистически независимыми* (относительно вероятности $\\mathrm{P}$), если\n",
    "$$ \\mathrm{P}(AB) = \\mathrm{P}(A) \\cdot \\mathrm{P}(B). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Теорема Байеса ###\n",
    "\n",
    "Из формул $\\mathrm{P}(B|A) = \\dfrac{\\mathrm{P}(AB)}{\\mathrm{P}(A)}$ и $\\mathrm{P}(A|B) = \\dfrac{\\mathrm{P}(AB)}{\\mathrm{P}(B)}$ получаем **формулу Байеса**:\n",
    "$$ \\mathrm{P}(A|B) = \\dfrac{\\mathrm{P}(B|A) \\mathrm{P}(A)}{\\mathrm{P}(B)}. $$\n",
    "\n",
    "Если события $A_1, \\dots, A_n$ образубт разбиение $\\Omega$, то из формул полной вероятности и Байеса следует **теорема Байеса**:\n",
    "$$ \\mathrm{P}(A_i|B) = \\frac{\\mathrm{P}(B|A_i) \\mathrm{P}(A_i)}{\\sum_{j=1}^{n} \\mathrm{P}(A_j) \\mathrm{P}(B|A_j)}. $$\n",
    "\n",
    "В статистических применениях события $A_1, \\dots, A_n$ образующие &laquo;полную группу событий&raquo; ($A_1 + \\dots + A_n = \\Omega$), часто называют &laquo;гипотезами&raquo;, а $\\mathrm{P}(A_i)$ &mdash; *априорной* вероятностью гипотезы $A_i$.\n",
    "Условная вероятность $\\mathrm{P}(A_i|B)$ трактуется как *апостериорная* вероятность гипотезы $A_i$ после наступления события $B$.\n",
    "\n",
    "> **Пример.**\n",
    "Пусть в урне находятся две монеты: $A_1$ &mdash; симметричная монета с вероятностью «герба» Г, равной 1/2, и $A_2$ &mdash; несимметричная монета с вероятностью «герба» Г, равной 1/3. Наудачу вынимается и подбрасывается одна из монет. Предположим, что выпал герб. Спрашивается, какова вероятность того, что выбранная монета симметрична."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Случайные величины и их распределения ##\n",
    "\n",
    "Пусть и $(\\Omega, \\mathcal{A}, \\mathrm{P})$ &mdash; вероятностная модель некоторого эксперимента с *конечным* числом исходов $N(\\Omega)$ и алгеброй $\\mathcal{A}$ всех подмножеств $\\Omega$.\n",
    "\n",
    "**Определение.** Всякая числовая функция $\\xi = \\xi(\\omega)$, определённая на (конечном) пространстве элементарных событий $\\Omega$, будет называться (простой) *случайной величиной*.\n",
    "\n",
    "Поскольку в рассматриваемом случае $\\Omega$ состоит из конечного числа точек, то множество значений $X$ случайной величины $\\xi$ также конечно.\n",
    "Пусть $X = \\{x_i, \\ldots, x_m\\}$, где (различными) числами $x_i, \\ldots, x_m$ исчерпываются все значения $\\xi$.\n",
    "\n",
    "Рассмотрим функцию вероятности $P_\\xi(\\cdot)$, индуцируемую случайной величиной $\\xi$ по формуле\n",
    "$$ P_\\xi(B) = \\mathrm{P}\\{\\omega: \\xi(\\omega) \\in B\\}. $$\n",
    "\n",
    "Ясно, что значения этих вероятностей полностью определяются вероятностями\n",
    "$$ P_\\xi(x_i) = \\mathrm{P}\\{\\omega: \\xi(\\omega) = x_i\\}, \\quad x_i \\in X. $$\n",
    "\n",
    "Набор чисел $\\{ P_\\xi(x_1), \\ldots , P_\\xi(x_m) \\}$ называется *распределением вероятностей* случайной величины $\\xi$.\n",
    "\n",
    "**Определение.** Пусть $x \\in \\mathbb{R}^1$. Функция\n",
    "$$ F_\\xi(x)  = \\mathrm{P} \\left\\{ \\omega: \\xi(\\omega) \\le x \\right\\} $$\n",
    "называется *функцией распределения* случайной величины $\\xi$.\n",
    "\n",
    "**Определение.** Случайные величины $\\xi_1, \\ldots, \\xi_r$ называются *независимыми* (в совокупности), если для любых $x_1, \\ldots, x_r \\in X$\n",
    "$$\n",
    "    \\mathrm{P}\\left\\{ \\xi_1=x_1, \\ldots, \\xi_r=x_r \\right\\} =\n",
    "    \\mathrm{P}\\left\\{ \\xi_1=x_1\\right\\} \\cdot \\ldots \\cdot \\mathrm{P}\\left\\{\\xi_r=x_r \\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Числовые характеристики случайных величин ##\n",
    "\n",
    "### Математическое ожидание ###\n",
    "\n",
    "**Определение.** *Математическим ожиданием* случайной величины $\\xi = \\xi(\\omega)$ называется число\n",
    "$$\n",
    "    \\mathrm{E}\\xi = \\sum\\limits_{i=1}^k x_i \\mathrm{P}(\\xi=x) = \\sum\\limits_{i=1}^k x_i P_\\xi(x_i).\n",
    "$$\n",
    "\n",
    "\n",
    "**Свойства математического ожидания:**\n",
    "\n",
    "1. Если $\\xi \\ge 0$, то $\\mathrm{E}\\xi \\ge 0$.\n",
    "2. $\\mathrm{E}(a\\xi +b\\eta) = a\\mathrm{E}\\xi +b\\mathrm{E}\\eta$, $\\hspace{0.5em}$ $a$, $b$ &mdash; постоянные (*линейность*).\n",
    "3. Если $\\xi \\ge \\eta$, то $\\mathrm{E}\\xi \\ge \\mathrm{E}\\eta$.\n",
    "4. $|\\mathrm{E}\\xi| \\le \\mathrm{E}|\\xi|$.\n",
    "5. Если $\\xi$ и $\\eta$ независимы, то $\\mathrm{E}\\xi\\eta = \\mathrm{E}\\xi \\cdot \\mathrm{E}\\eta$.\n",
    "6. $(\\mathrm{E}|\\xi\\eta|)^2 \\le \\mathrm{E}\\xi^2 \\cdot \\mathrm{E}\\eta^2$ (*неравенство Коши&ndash;Буняковского&ndash;Шварца*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Дисперсия ###\n",
    "\n",
    "**Определение.** *Дисперсией* случайной величины $\\xi$ называется величина\n",
    "$$ \\mathrm{D} \\xi = \\mathrm{E} \\left( \\xi - \\mathrm{E} \\xi \\right)^2. $$\n",
    "\n",
    "Величина $\\sigma_\\xi = +\\sqrt{\\mathrm{D} \\xi}$ называется *стандартным отклонением* значений случайной величины $\\xi$ от её среднего значения $\\mathrm{E} \\xi$.\n",
    "\n",
    "**Свойства дисперсии:**\n",
    "\n",
    "1. Дисперсию случайной величины $\\xi$ можно вычислить как разность математического ожидания кавдрата величины и квадрата математического ожидания: $\\mathrm{D}\\xi = \\mathrm{E} \\xi^2 - \\left( \\mathrm{E} \\xi \\right)^2$.\n",
    "2. $\\mathrm{D}\\xi \\ge 0$.\n",
    "3. $\\mathrm{D}(a + b\\xi) = b^2 \\mathrm{D}\\xi$, $\\hspace{0.5em}$ $a$, $b$ &mdash; постоянные.\n",
    "4. $\\mathrm{D}(\\xi + \\eta) = \\mathrm{E} \\left[ (\\xi-\\mathrm{E}\\xi) + (\\eta-\\mathrm{E}\\eta) \\right]^2 = \\mathrm{D}\\xi + \\mathrm{D}\\eta + 2\\mathrm{E}(\\xi-\\mathrm{E}\\xi)(\\eta-\\mathrm{E}\\eta)$\n",
    "\n",
    "\n",
    "**Определение.** Пусть $\\xi$ и  $\\eta$ &mdash; две случайные величины. Их *ковариацией* называется величина\n",
    "$$\n",
    "    \\mathrm{cov}(\\xi, \\eta) = \\mathrm{E} (\\xi-\\mathrm{E}\\xi)(\\eta-\\mathrm{E}\\eta).\n",
    "$$\n",
    "\n",
    "С учётом введённого обозначения для ковариации находим, что\n",
    "$$ \\mathrm{D}(\\xi+\\eta) = \\mathrm{D}\\xi  + \\mathrm{D}\\eta +  2\\mathrm{cov}(\\xi, \\eta).$$\n",
    "\n",
    "Если $\\mathrm{cov}\\left( \\xi, \\eta \\right) = 0$, то говорят, что случайные величины $\\xi$ и $\\eta$ *некоррелированы*. \\\n",
    "Если $\\xi$ и $\\eta$ некоррелированы, то дисперсия суммы $\\mathrm{D}(\\xi+\\eta)$ равна сумме дисперсий:\n",
    "$$ \\mathrm{D}(\\xi+\\eta) = \\mathrm{D}\\xi + \\mathrm{D}\\eta. $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Замечание.** Из некоррелированности $\\xi$ и $\\eta$, вообще говоря, не следует их независимость. Проиллюстрируем этот факт следующим примером.\n",
    "\n",
    "> **Пример.** Пусть случайная величина $\\alpha$ принимает значения 0, $\\pi/2$ и $\\pi$ с вероятностями 1/3. Рассмотрим две случайные величины $\\xi = \\sin \\alpha$ и $\\eta = \\cos \\alpha$. \\\n",
    "Величины $\\xi$ и $\\eta$ некоррелированы, однако они не только зависимы относительно вероятности, но и *функционально зависимы*: $\\xi^2 + \\eta^2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Коэффициент корреляции ###\n",
    "\n",
    "**Определение.** Если $\\mathrm{D}\\xi > 0$, $\\mathrm{D}\\eta > 0$, то величина\n",
    "$$\n",
    "    \\rho(\\xi, \\eta) = \\dfrac{\\mathrm{cov}(\\xi, \\eta)}{\\sqrt{\\mathrm{D}\\xi \\cdot \\mathrm{D}\\eta}} = \\dfrac{\\mathrm{cov}(\\xi, \\eta)}{\\sigma_\\xi \\cdot \\sigma_\\eta}\n",
    "$$\n",
    "называется *коэффициентом корреляции* случайных величин $\\xi$ и $\\eta$.\n",
    "\n",
    "Eсли $\\rho(\\xi, \\eta) = \\pm 1$, то величины $\\xi$ и $\\eta$ линейно зависимы:\n",
    "$$ \\eta =a \\xi + b, $$\n",
    "где $a>0$, если $\\rho(\\xi, \\eta) = 1$, $a<0$, если $\\rho(\\xi, \\eta) = -1$.\n",
    "\n",
    "**Определение.** Говорят, что $\\xi$ и $\\eta$ отрицательно коррелированы, если $\\rho(\\xi, \\eta) < 0$; положительно коррелированы, если $\\rho(\\xi, \\eta) > 0$; некоррелированы, если $\\rho(\\xi, \\eta) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Оптимальная оценка случайных величин ##\n",
    "\n",
    "Рассмотрим две случайные величины $\\xi$ и $\\eta$. Предположим, что наблюдению подлежит лишь случайная величина $\\xi$.\n",
    "Если величины $\\xi$ и $\\eta$ коррелированы, то можно ожидать, что знание значений $\\xi$ позволит вынести некоторые суждения и о значениях ненаблюдаемой величины $\\eta$.\n",
    "\n",
    "Всякую функцию $f = f(\\xi)$ от $\\xi$ будем называть *оценкой* для $\\eta$.\n",
    "Будем говорить также, что *оценка* $f^\\ast = f^\\ast(\\xi)$ *оптимальна в среднеквадратическом смысле*, если\n",
    "$$ \\mathrm{E}(\\eta − f^\\ast(\\xi))^2 = \\inf_f \\mathrm{E}(\\eta − f(\\xi))^2. $$\n",
    "\n",
    "Покажем, как найти оптимальную оценку в классе *линейных* оценок $\\lambda(\\xi) = a + b\\xi$.\n",
    "Для этого рассмотрим функцию $g(a, b) = \\mathrm{E}(\\eta − (a+b\\xi))^2$.\n",
    "Дифференцируя $g(a, b)$ по $a$ и $b$, получаем\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{\\partial g(a, b)}{\\partial a} &= −2 \\mathrm{E} \\left[ \\eta − (a+b\\xi) \\right], \\\\\n",
    "    \\frac{\\partial g(a, b)}{\\partial b} &= −2 \\mathrm{E} \\left[ (\\eta − (a+b\\xi))\\xi \\right],\n",
    "\\end{split}\n",
    "$$\n",
    "откуда, приравнивая производные к нулю, находим, что *оптимальная* в среднеквадратическом смысле *линейная* оценка есть $\\lambda^\\ast (\\xi) = a^\\ast + b^\\ast \\xi$, где\n",
    "$$\n",
    "    a^\\ast = \\mathrm{E}\\eta − b^\\ast\\mathrm{E}\\xi, \\quad b^\\ast = \\frac{\\mathrm{cov}(\\xi, \\eta)}{\\mathrm{D}\\xi}.\n",
    "$$\n",
    "\n",
    "Иначе говоря,\n",
    "$$\n",
    "    \\lambda^\\ast(\\xi) = \\mathrm{E}\\eta + \\frac{\\mathrm{cov}(\\xi, \\eta)}{\\mathrm{D}\\xi} (\\xi - \\mathrm{E}\\xi).\n",
    "$$\n",
    "\n",
    "Величина $\\mathrm{E}(\\eta − \\lambda^\\ast(\\xi))^2$ называется *среднеквадратической ошибкой* оценивания.\n",
    "Простой подсчёт показывает, что эта ошибка равна\n",
    "$$\n",
    "\\Delta^\\ast = \\mathrm{E}(\\eta − \\lambda^\\ast(\\xi))^2 = \\mathrm{D}\\eta − \\frac{\\mathrm{cov}^2(\\xi, \\eta)}{\\mathrm{D}\\xi} = \\mathrm{D}\\eta \\cdot [1 - \\rho^2(\\xi, \\eta)].\n",
    "$$\n",
    "\n",
    "Таким образом, чем больше по модулю коэффициент корреляции $\\rho(\\xi, \\eta)$ между $\\xi$ и $\\eta$, тем меньше среднеквадратическая ошибка оценивания $\\Delta^\\ast$. В частности, если $|\\rho(\\xi, \\eta)|=1$, то $\\Delta^\\ast = 0$. Если же случайные величины $\\xi$ и $\\eta$ не коррелированы (т. е. $\\rho(\\xi, \\eta)=0$), то $\\lambda^\\ast(\\xi) = \\mathrm{E}\\eta$. Таким образом, в случае отсутствия корреляции между $\\xi$ и $\\eta$ лучшей оценкой $\\eta$ по $\\xi$ является просто $\\mathrm{E}\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Литература ##\n",
    "\n",
    "1. Ширяев А.Н. Вероятность &mdash; 1. М.: МЦНМО, 2007. 517 с.\n",
    "1. Чернова Н. И. Теория вероятностей. Учебное пособие. Новосибирск, 2007. 160 с.\n",
    "1. Феллер В. Введение в теорию вероятностей и её приложения. М.: Мир, 1964. 498 с."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
