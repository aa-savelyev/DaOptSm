# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.13.7
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# + [markdown] slideshow={"slide_type": "slide"}
# **Лекция 2**
#
# # Аксиоматика теории вероятностей

# + [markdown] toc=true
# <h1>Содержание<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Предмет-теории-вероятностей" data-toc-modified-id="Предмет-теории-вероятностей-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Предмет теории вероятностей</a></span></li><li><span><a href="#Вероятностная-модель" data-toc-modified-id="Вероятностная-модель-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Вероятностная модель</a></span><ul class="toc-item"><li><span><a href="#Пространство-элементарных-исходов" data-toc-modified-id="Пространство-элементарных-исходов-2.1"><span class="toc-item-num">2.1&nbsp;&nbsp;</span>Пространство элементарных исходов</a></span></li><li><span><a href="#Алгебра-и-${\sigma}$-алгебра-событий" data-toc-modified-id="Алгебра-и-${\sigma}$-алгебра-событий-2.2"><span class="toc-item-num">2.2&nbsp;&nbsp;</span>Алгебра и ${\sigma}$-алгебра событий</a></span></li><li><span><a href="#Вероятностная-мера" data-toc-modified-id="Вероятностная-мера-2.3"><span class="toc-item-num">2.3&nbsp;&nbsp;</span>Вероятностная мера</a></span></li><li><span><a href="#Вероятностное-пространство" data-toc-modified-id="Вероятностное-пространство-2.4"><span class="toc-item-num">2.4&nbsp;&nbsp;</span>Вероятностное пространство</a></span></li><li><span><a href="#Замечания" data-toc-modified-id="Замечания-2.5"><span class="toc-item-num">2.5&nbsp;&nbsp;</span>Замечания</a></span><ul class="toc-item"><li><span><a href="#Построение-вероятностного-пространства" data-toc-modified-id="Построение-вероятностного-пространства-2.5.1"><span class="toc-item-num">2.5.1&nbsp;&nbsp;</span>Построение вероятностного пространства</a></span></li><li><span><a href="#Так-что-же-такое-вероятность?" data-toc-modified-id="Так-что-же-такое-вероятность?-2.5.2"><span class="toc-item-num">2.5.2&nbsp;&nbsp;</span>Так что же такое вероятность?</a></span></li></ul></li></ul></li><li><span><a href="#Примеры" data-toc-modified-id="Примеры-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Примеры</a></span><ul class="toc-item"><li><span><a href="#Биномиальное-распределение" data-toc-modified-id="Биномиальное-распределение-3.1"><span class="toc-item-num">3.1&nbsp;&nbsp;</span>Биномиальное распределение</a></span></li><li><span><a href="#Гипергеометрическое-распределение" data-toc-modified-id="Гипергеометрическое-распределение-3.2"><span class="toc-item-num">3.2&nbsp;&nbsp;</span>Гипергеометрическое распределение</a></span></li></ul></li><li><span><a href="#Оценка-максимального-правдоподобия" data-toc-modified-id="Оценка-максимального-правдоподобия-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Оценка максимального правдоподобия</a></span><ul class="toc-item"><li><span><a href="#Задача-об-оценке-генеральной-совокупности-по-выборке" data-toc-modified-id="Задача-об-оценке-генеральной-совокупности-по-выборке-4.1"><span class="toc-item-num">4.1&nbsp;&nbsp;</span>Задача об оценке генеральной совокупности по выборке</a></span></li></ul></li><li><span><a href="#Источники" data-toc-modified-id="Источники-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Источники</a></span></li></ul></div>
# -

# Imports
import numpy as np
import matplotlib.pyplot as plt

# +
# Styles
import matplotlib
matplotlib.rcParams['font.size'] = 12
matplotlib.rcParams['lines.linewidth'] = 1.5
matplotlib.rcParams['lines.markersize'] = 4
cm = matplotlib.pyplot.cm.tab10  # Colormap

import seaborn
seaborn.set_style('whitegrid')

# +
# # %config InlineBackend.figure_formats = ['pdf']
# # %config Completer.use_jedi = False
# -

# ---

# + [markdown] slideshow={"slide_type": "slide"}
# ## Предмет теории вероятностей
#
# Предметом теории вероятностей является математический анализ случайных явлений &mdash; эмпирических феноменов, которые (при заданном &laquo;комплексе условий&raquo;) могут быть охарактеризованы тем, что
#
# - для них отсутствует *детерминистическая регулярность* (наблюдения над ними не всегда приводят к одним и тем же исходам)
#
# и в то же самое время
#
# - они обладают некоторой *статистической регулярностью* (проявляющейся в статистической устойчивости частот).
#
# Поясним сказанное на классическом примере &laquo;честного&raquo; подбрасывания &laquo;правильной&raquo; монеты.
# Ясно, что заранее невозможно с определённостью предсказать исход каждого подбрасывания.
# Результаты отдельных экспериментов носят крайне нерегулярный характер (то &laquo;герб&raquo;, то &laquo;решетка&raquo;), и кажется, что это лишает нас возможности познать какие-либо закономерности, связанные с этими экспериментами.
# Однако, если провести большое число &laquo;независимых&raquo; подбрасываний, то можно заметить, что для &laquo;правильной&raquo; монеты будет наблюдаться вполне определенная статистической регулярность, проявляющаяся в том, что частота выпадания &laquo;герба&raquo; будет &laquo;близка&raquo; к $1/2$.

# + [markdown] slideshow={"slide_type": "skip"}
# ---
# -

# ## Вероятностная модель
#
# Согласно аксиоматике Колмогорова первоначальным объектом теории вероятностей является *вероятностное пространство* $(\Omega, \mathcal{F}, \mathrm{P})$.
# Здесь $\Omega$ &mdash; это множество, состоящее из элементарных событий $\omega$, с выделенной на нём системой его подмножеств (событий) $\mathcal{F}$, образующих $\sigma$-алгебру, а $\mathrm{P}$ &mdash; вероятностная мера (вероятность), определённая на множествах из $\mathcal{F}$.

# + [markdown] slideshow={"slide_type": "notes"}
# ### Пространство элементарных исходов
#
# **Определение**. *Пространством элементарных исходов* называется множество $\Omega$, содержащее все возможные *взаимоисключающие* результаты данного случайного эксперимента.
# Элементы множества $\Omega$ называются элементарными исходами и обозначаются буквой $\omega$.
#
# Элементарный исход &mdash; это мельчайший неделимый результат эксперимента. \
# Выделение пространства элементарных исходов представляет собой первый шаг в формулировании понятия *вероятностной модели* (вероятностной &laquo;теории&raquo;) того или иного эксперимента.

# + [markdown] slideshow={"slide_type": "notes"}
# **Примеры:**
#
# - однократное подбрасывание монеты (пространство исходов состоит из двух точек: Г &mdash; &laquo;герб&raquo;, Р &mdash; &laquo;решетка&raquo;);
# - n-кратное подбрасывание монеты;
# - выбор шаров с возвращением (упорядоченные и неупорядоченные выборки);
# - выбор шаров без возвращения (упорядоченные и неупорядоченные выборки).

# + [markdown] slideshow={"slide_type": "notes"}
# ### Алгебра и ${\sigma}$-алгебра событий
#
# Наряду с понятием пространства элементарных исходов введём теперь важное понятие события, лежащее в основе построения всякой
# вероятностной модели рассматриваемого эксперимента.
#
# Мы собираемся определить набор подмножеств множества $\Omega$, которые будут называться событиями, и затем задать вероятность как функцию, определённую *только* на множестве событий.
#
# *Событиями* мы будем называть не любые подмножества $\Omega$, а лишь элементы некоторого выделенного набора подмножеств множества $\Omega$.
# В качестве наборов событий целесообразно рассматривать системы множеств, являющиеся *алгебрами*.
# Для этого необходимо позаботиться, чтобы этот набор подмножеств был *замкнут* относительно операций объединения, пересечения и дополнения.
#
# **Определение.** Множество $\mathcal{F}$, элементами которого являются подмножества множества $\Omega$ называется $\sigma$-*алгеброй*, если оно удовлетворяет следующим аксиомам:
#
# 1. $\Omega \in \mathcal{F}$ ($\sigma$-алгебра содержит *достоверное событие*);
# 1. если $A \in \mathcal{F}$, то $\overline{A} \in \mathcal{F}$ (вместе с любым множеством $\sigma$-алгебра содержит противоположное к нему);
# 1. если $A_1,\,A_2,\,\ldots \in \mathcal{F}$, то $\bigcup\limits_{i=1}^{\infty}A_i \in \mathcal{F}$ (вместе с любым *счётным* набором событий $\sigma$-алгебра содержит их объединение).
# -

# **Свойства:**
#
# 1. Из аксиом 1 и 2 следует, что пустое множество $\emptyset = \overline{\Omega}$ также содержится в $\mathcal{F}$, т. е. алгебра содержит и *невозможное событие*.
# 1. Из аксиом 2 и 3 следует, что вместе с любым счётным набором событий $\sigma$-алгебра содержит не только их объединение $\bigcup\limits_{i=1}^{\infty}A_i$, но и их пересечение $\bigcap\limits_{i=1}^{\infty}A_i$.
#
# **Примеры:**
#
# 1. $\mathcal{F} = \{ \Omega, \emptyset \}$ &mdash; система, состоящая из $\Omega$ и пустого множества (так называемая тривиальная алгебра);
# 1. $\mathcal{F} = \{ A, \overline{A}, \Omega, \emptyset \}$ &mdash; система, порождённая событием $A$;
# 1. $\mathcal{F} = \{ A: A \subseteq \Omega \}$ &mdash; совокупность всех подмножеств $\Omega$ (обозначается $2^\Omega$).
#
# > *Упражнение.* Доказать, что если $\Omega$ состоит из $n$ элементов, то в множестве всех его подмножеств ровно $2^n$ элементов.

# + [markdown] slideshow={"slide_type": "notes"}
# ### Вероятностная мера
#
# Пока мы сделали два первых шага к построению вероятностной модели эксперимента: выделили пространство исходов $\Omega$ и некоторую систему $\mathcal{F}$ его подмножеств, образующих $\sigma$-алгебру и называемых событиями.
# Сделаем теперь следующий шаг, а именно введём вероятностную меру.
#
# **Определение.** Пусть $\Omega$ &mdash; непустое множество, а $\mathcal{F}$ &mdash; $\sigma$-алгебра его подмножеств. Функция $\mathrm{P}: \mathcal{F} \rightarrow \mathbb{R}$ называется *вероятностной мерой*, если она удовлетворяет следующим аксиомам:
#
# 1. $\mathrm{P}(A) \ge 0$, $A \in \mathcal{F}$ (неотрицательность);
# 1. $\mathrm{P}\left( \bigcup\limits_{i=1}^{\infty}A_i \right) = \sum\limits_{i=1}^{\infty}\mathrm{P}(A_i)$,
# где $A_i \in \mathcal{F}$, $A_i \cap A_j = \emptyset$, $i \ne j$ (счётная или $\sigma$-аддитивность) \
#    (для любого счётного набора попарно несовместных событий мера их объединения равна сумме их мер);
# 1. $\mathrm{P}(\Omega) = 1$ (нормированность).
#
# **Замечание 1.** Аксиомы 1 и 2 задают *меру* как неотрицательную $\sigma$-аддитивную функцию множеств, аксиома 3 определяет вероятность как *нормированную меру*.
#
# **Замечание 2.** Существуют примеры неизмеримых множеств, например, *множество Витали*.
# -

# **Свойства:**
#
# 1. $\mathrm{P}: \mathcal{F} \rightarrow [0,1]$
# 1. $\mathrm{P}(\emptyset) = 0$;
# 1. $\mathrm{P}(\overline{A}) = 1 - \mathrm{P}(A)$;
# 1. $\mathrm{P}(A \cup B) = \mathrm{P}(A) + \mathrm{P}(B) - \mathrm{P}(A \cap B)$;
# 1. $\mathrm{P}\left( \bigcup\limits_{i=1}^{n}A_i \right) = \sum\limits_{i}\mathrm{P}(A_i) - \sum\limits_{i<j}\mathrm{P}(A_i \cap A_j) + \sum\limits_{i<j<k}\mathrm{P}(A_i \cap A_j \cap A_k) + \ldots + (-1)^{n-1} \mathrm{P}\left( \bigcap\limits_{i=1}^{n}A_i \right)$ &mdash; формула включения-исключения.

# + [markdown] slideshow={"slide_type": "subslide"}
# ### Вероятностное пространство
#
# **Определение.** Тройка объектов
#
# $$ \left( \Omega, \mathcal{F}, \mathrm{P} \right), $$
#
# где $\Omega$ &mdash; множество элементарных исходов, $\mathcal{F}$ &mdash; $\sigma$-алгебра его подмножеств и $\mathrm{P}$ &mdash; вероятностная мера на $\mathcal{F}$, называется *вероятностным пространством*.

# + [markdown] slideshow={"slide_type": "notes"}
# ### Замечания
#
# #### Построение вероятностного пространства
#
# При построении вероятностных моделей в конкретных ситуациях выделение пространства элементарных событий $\Omega$ и алгебры событий $\mathcal{F}$, как правило, не является сложной задачей.
# При этом в элементарной теории вероятностей в качестве алгебры $\mathcal{F}$ обычно берется алгебра *всех* подмножеств $\Omega$.
# Труднее обстоит дело с вопросом о том, как задавать вероятности элементарных событий.
# В сущности, ответ на этот вопрос лежит вне рамок теории вероятностей, и мы его подробно не рассматриваем, считая, что основной нашей задачей является не вопрос о том, как приписывать исходам те или иные вероятности, а *вычисление* вероятностей
# сложных событий (событий из $\mathcal{F}$) по вероятностям элементарных событий.
#
# С математической точки зрения ясно, что в случае конечного пространства элементарных событий с помощью приписывания исходам $\omega_1, \ldots , \omega_N$ неотрицательных чисел $p_1, \ldots , p_N$, удовлетворяющих условию $p_1 + \ldots + p_N = 1$, мы получаем все мыслимые (конечные) вероятностные пространства.
#
# *Правильность* же назначенных для конкретной ситуации значений $p_1, \ldots , p_N$ может быть до известной степени проверена с помощью *закона больших чисел*, согласно которому в длинных сериях &laquo;независимых&raquo; экспериментов, происходящих при одинаковых условиях, частоты появления элементарных событий &laquo;близки&raquo; к их вероятностям.
# -

# #### Так что же такое вероятность?
#
# **Классическое определение вероятности.**
# Это то, чему нас учат в школе.
# Оно основано на симметрии монет, костей, перетасованных колод карт и так далее и может быть сформулировано как &laquo;отношение числа благоприятных исходов к числу всех исходов, если все исходы равновозможны&raquo;.
# Например, вероятность выпадения единицы на правильной кости равна 1/6, потому что возможны 6 исходов, а нас
# устраивает один.
# Однако это определение в какой-то степени носит круговой характер, поскольку прежде мы должны уяснить, что значит
# равновозможны.
#
# **&laquo;Перечислительная&raquo; вероятность.**
# Предположим, в ящике лежат три белых и четыре черных носка.
# Если вытаскивать носок случайным образом, то чему равна вероятность, что он белый?
# Ответ 3/7 можно получить путем простого перечисления всех возможностей.
# Многие из нас страдали от таких вопросов в школе, и здесь мы фактически имеем дело с расширением рассмотренной выше
# классической идеи, где требуется случайный выбор из группы физических объектов.
# Мы уже использовали эту идею при описании случайного выбора элемента данных из общей генеральной совокупности.
#
# **Вероятность как частота.**
# Такое определение говорит о вероятности как о доле случаев, когда интересующее нас событие наступает в бесконечной последовательности идентичных экспериментов.
# Для бесконечно повторяющихся событий это может быть разумно (хотя бы теоретически), но как насчет уникальных одноразовых событий, например скачек или завтрашней погоды?
# На деле практически любая реальная ситуация даже в принципе не может быть бесконечно воспроизводимой.
#
# **Субъективная, или &laquo;личная&raquo;, вероятность.**
# Это степень веры конкретного человека в какое-либо событие, основанная на его нынешних знаниях.
# Субъективная вероятность означает, что любая численная вероятность фактически *строится* в соответствии с тем, что известно в нынешней ситуации, &mdash; и на самом деле вероятность вообще не &laquo;существует&raquo; (за исключением, возможно, субатомного уровня).
# Такой подход лежит в основе **байесовской** школы статистики.

# + [markdown] slideshow={"slide_type": "skip"}
# ---

# + [markdown] slideshow={"slide_type": "notes"}
# ## Примеры
#
# ### Биномиальное распределение
#
# Предположим, что монета подбрасывается $n$ раз и результат наблюдений записывается в виде упорядоченного
# набора $(a_1, \ldots, a_n)$, где $a_i = 1$ в случае появления «герба» («успех») и $a_i = 0$ в случае появления «решетки» («неуспех»).
# Пространство всех исходов имеет следующую структуру:
# $$ \Omega= \left\{ \omega: \omega = (a_1, \ldots, a_n), \quad a_i = 0 \; \mathrm{или} \; 1 \right\}. $$
#
# Припишем каждому элементарному событию $\omega = (a_1, \ldots, a_n)$ вероятность (&laquo;вес&raquo;)
# $$ p(\omega) = p^{\sum a_i} q^{n-\sum a_i}, $$
# где неотрицательные числа $p$ и $q$ таковы, что $p + q = 1$.
#
# Итак, пространство $\Omega$ вместе с системой $\mathcal{A}$ всех его подмножеств и вероятностями $\mathrm{P}(A) = \sum\limits_{\omega \in A}p(\omega), \; A \in \mathcal{A}$ (в частности, $\mathrm{P}(\{\omega\}) = p(\omega), \; \omega \in \Omega$) определяет некоторую вероятностную модель.
# Естественно назвать её *вероятностной моделью, описывающей $n$-кратное подбрасывание монеты*.
#
# Введём в рассмотрение события
# $$ 
#     A_k = \left\{\omega: \omega=(a_1, \ldots, a_n), a_1 + \ldots + a_n = k\right\}, \quad k = 0, 1, \ldots, n,
# $$
# означающие, что произойдет в точности $k$ &laquo;успехов&raquo;. Тогда вероятность события $A_k$ равна
# $$ \mathrm{P}(A_k) = C_n^k p^k q^{n-k}, $$
# причём $\sum\limits_{k=0}^n \mathrm{P}(A_k) = 1$.
#
# Набор вероятностей $\{\mathrm{P}(A_0), \ldots,\mathrm{P}(A_n)\}$ называется *биномиальным распределением* (числа &laquo;успехов&raquo; в выборке объёма $n$).

# + [markdown] slideshow={"slide_type": "subslide"}
# ### Гипергеометрическое распределение
#
# Рассмотрим урну, содержащую $N$ шаров, из которых $M$ шаров имеют белый цвет.
# Предположим, что осуществляется выбор без возвращения объёма $n < N$.
# Вероятность события $B_m$, состоящего в том, что $m$ шаров из выборки имеют белый цвет равна
#
# $$ \mathrm{P}(B_m) = \dfrac{C_M^m C_{N-M}^{n-m}}{C_N^n}. $$
#
# Набор вероятностей $\{\mathrm{P}(B_0), \ldots,\mathrm{P}(B_n)\}$ носит название многомерного гипергеометрического распределения.
# -

# ## Оценка максимального правдоподобия
#
# Пусть $N$ &mdash; размер некоторой популяции, который требуется оценить &laquo;минимальными средствами&raquo; без простого пересчета всех элементов этой совокупности.
# Подобного рода вопрос интересен, например, при оценке числа жителей в той или иной стране, городе и т. д.
#
# В 1786 г. Лаплас для оценки числа $N$ жителей Франции предложил следующий метод.
#
# 1. Выберем некоторое число $M$, элементов популяции и пометим их.
# 2. Возвратим их в основную совокупность и предположим, что они &laquo;хорошо перемешаны&raquo; с немаркированными элементами.
# 3. После этого возьмём из &laquo;перемешанной&raquo; популяции $n$ элементов.
# 4. Пусть среди них $m$ элементов оказались маркированными.
#
# Вероятность $\mathrm{P}(B_m(N))$ задается формулой гипергеометрического распределения:
# $$
#     \mathrm{P}(B_m(N)) = \frac{C_M^m C_{N-M}^{n-m}}{C_N^n}. \tag{1}\label{eq:prob}
# $$
#
# Нам известны числа $M$, $n$ и $m$, а $N$ (размер популяции) &mdash; нет, его требуется оценить.
#
# Для каждого частного набора наблюдений $M$, $n$ и $m$ значение $N$, при котором вероятность $\mathrm{P}(B_m(N))$ максимальна, называется **оценкой максимального правдоподобия**.
# Обозначим наиболее правдоподобное значение через $\hat{N}$.
#
# Можно показать, что $\hat{N}$ определяется следующей формулой ($[\cdot]$ &mdash; целая часть):
# $$ \hat{N} = \left[\dfrac{Mn}{m}\right]. \tag{2}\label{eq:max} $$
#
#
# >*Задание.* Получить формулу $\eqref{eq:max}$. \
# Подсказка: можно воспользоваться формулой Стирлинга $n! \sim \sqrt{2 \pi n}\left( \dfrac{n}{e} \right)^n$.

# ### Задача об оценке генеральной совокупности по выборке
#
# Применим метод максимального правдоподобия для оценки количества рыб в озере.
# Пусть, например, $M=1000$, $n=1000$, а $m=100$.
# Тогда всё, что нам достоверно известно о количестве рыб, это $N \ge n + M - m = 1900$.
# Вообще говоря, не исключено, что в озере их ровно $1900$.
# Однако, отправляясь от этой гипотезы, мы придём к выводу, что случилось событие фантастически малой вероятности.
# Действительно, вероятность того, что выборка объёмом $n=1000$ из генеральной совокупности объёма $N=1900$ будет содержать $m=100$ маркированных объектов, если общее число маркированных объектов $M=1000$, по формуле ([1](#mjx-eqn-eq:prob)) равна
# $$
#     \mathrm{P}(B_{100}(1900)) = \frac{C_{1000}^{100} C_{900}^{900}}{C_{1900}^{1000}} = \frac{(1000!)^2}{100! \, 1900!} \sim 10^{-430}.
# $$
#
# Аналогичное рассуждение заставляет нас откинуть гипотезу о том, что $N$ очень велико, скажем равно миллиону ($\mathrm{P}(B_{100}(10^6)) \sim 10^{-163}$).
#

# +
import mpmath
from scipy.stats import hypergeom

P_1e2 = mpmath.fac(1000)**2 / mpmath.fac(100) / mpmath.fac(1900)
print(P_1e2)

P_1e6 = hypergeom.pmf(100, 1e6, 1000, 1000)
print(P_1e6)


# -

# Обозначим вероятность события $B_{100}(N)$ через $P(N)$ и построим её зависимость от $N$.

# +
def P(x):
    return hypergeom.pmf(100, x, 1000, 1000)

# Generate data
X = np.arange(5000, 28000, 100)
Y = P(X)
# -

# Show data
plt.figure(figsize=(8, 5))
plt.plot(X, Y, '-')
plt.yscale('log')
plt.xlabel('$N$')
plt.ylabel('$P(N)$', rotation=0, ha='right')
plt.show()

# +
# Find maximum likelihood estimation
mle_idx = np.argmax(Y)
x_mle = X[mle_idx]
y_mle = Y[mle_idx]

print(f'P({x_mle}) = {y_mle:.3}')
# -

# В нашем примере оценкой максимального правдоподобия для количества рыб в озере является число $\hat{N} = 10^4$, а вероятность соответствующего события $\mathrm{P}(B_{100}(10^4)) \approx 0.044$.

# + [markdown] slideshow={"slide_type": "skip"}
# ---

# + [markdown] slideshow={"slide_type": "slide"}
# ## Источники
#
# 1. *Ширяев А.Н.* Вероятность &mdash; 1. &mdash; М.: МЦНМО, 2007. &mdash; 517 с.
# 1. *Чернова Н.И.* Теория вероятностей. Учебное пособие. &mdash; Новосиб. гос. ун-т, 2007. &mdash; 160 с.
# 1. *Феллер В.* Введение в теорию вероятностей и её приложения. &mdash; М.: Мир, 1964. &mdash; 498 с.
# 1. *Шпигельхалтер Д.* Искусство статистики. Как находить ответы в данных. &mdash; М.: Манн, Иванов и Фербер, 2021. &mdash; 448 с.
# -
# Versions used
import sys
print('Python: {}.{}.{}'.format(*sys.version_info[:3]))
print('numpy: {}'.format(np.__version__))
print('matplotlib: {}'.format(matplotlib.__version__))
print('seaborn: {}'.format(seaborn.__version__))

