{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RW #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preface\" data-toc-modified-id=\"Preface-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preface</a></span></li><li><span><a href=\"#Предисловие\" data-toc-modified-id=\"Предисловие-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Предисловие</a></span></li><li><span><a href=\"#Chapter-1.-Introduction\" data-toc-modified-id=\"Chapter-1.-Introduction-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Chapter 1. Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-A-Pictorial-Introduction-to-Bayesian-Modelling\" data-toc-modified-id=\"1.1-A-Pictorial-Introduction-to-Bayesian-Modelling-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>1.1 A Pictorial Introduction to Bayesian Modelling</a></span></li><li><span><a href=\"#1.1-Пикториальное-введение-в-байесовское-моделирование\" data-toc-modified-id=\"1.1-Пикториальное-введение-в-байесовское-моделирование-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>1.1 Пикториальное введение в байесовское моделирование</a></span></li></ul></li><li><span><a href=\"#Chapter-2.-Regression\" data-toc-modified-id=\"Chapter-2.-Regression-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Chapter 2. Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Weight-space-View\" data-toc-modified-id=\"2.1-Weight-space-View-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>2.1 Weight-space View</a></span></li><li><span><a href=\"#2.1-Весовое-пространство\" data-toc-modified-id=\"2.1-Весовое-пространство-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>2.1 Весовое пространство</a></span></li><li><span><a href=\"#2.2-Function-space-View\" data-toc-modified-id=\"2.2-Function-space-View-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>2.2 Function-space View</a></span></li><li><span><a href=\"#2.2-Функционально-пространственный-просмотр\" data-toc-modified-id=\"2.2-Функционально-пространственный-просмотр-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>2.2 Функционально-пространственный просмотр</a></span></li><li><span><a href=\"#2.3-Varying-the-Hyperparameters\" data-toc-modified-id=\"2.3-Varying-the-Hyperparameters-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>2.3 Varying the Hyperparameters</a></span></li><li><span><a href=\"#2.3-Изменение-гиперпараметров\" data-toc-modified-id=\"2.3-Изменение-гиперпараметров-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>2.3 Изменение гиперпараметров</a></span></li></ul></li><li><span><a href=\"#Chapter-4.-Covariance-Functions\" data-toc-modified-id=\"Chapter-4.-Covariance-Functions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Chapter 4. Covariance Functions</a></span></li><li><span><a href=\"#Chapter-5.-Model-Selection-and-Adaptation-of-Hyperparameters\" data-toc-modified-id=\"Chapter-5.-Model-Selection-and-Adaptation-of-Hyperparameters-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Chapter 5. Model Selection and Adaptation of Hyperparameters</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface ##\n",
    "## Предисловие ##\n",
    "\n",
    "Roughly speaking a stochastic process is a generalization of a probability Gaussian process distribution (which describes a finite-dimensional random variable) to functions. By focussing on processes which are Gaussian, it turns out that the computations required for inference and learning become relatively easy. Thus, the supervised learning problems in machine learning which can be thought of as learning a function from examples can be cast directly into the Gaussian process framework.\n",
    "\n",
    "Грубо говоря, стохастический процесс - это обобщение распределения вероятности Гауссовского процесса (описывающего конечно-размерную случайную величину) на функции. Сосредоточившись на процессах, которые являются гауссовскими, получается, что вычисления, необходимые для вывода и изучения, становятся относительно простыми. Таким образом, контролируемые проблемы обучения в машинном обучении, которые можно рассматривать как изучение функции на примерах, могут быть брошены непосредственно в гауссовские рамки процесса.\n",
    "\n",
    "Many researchers were realizing that neural networks were not so easy to neural networks apply in practice, due to the many decisions which needed to be made: what architecture, what activation functions, what learning rate, etc., and the lack of a principled framework to answer these questions. The probabilistic framework was pursued using approximations by MacKay [1992b] and using Markov chain Monte Carlo (MCMC) methods by Neal [1996]. Neal was also a graduate student in the same lab, and in his thesis he sought to demonstrate that using the Bayesian formalism, one does not necessarily have problems with “overfitting” when the models get large, and one should pursue the limit of large models. While his own work was focused on sophisticated Markov chain methods for inference in large finite networks, he did point out that some of his networks became Gaussian processes in the limit of infinite size, and “there may be simpler ways to do inference in this case.”\n",
    "\n",
    "Многие исследователи понимали, что нейронные сети не так легко применять на практике из-за множества решений, которые необходимо было принять: какая архитектура, какие функции активации, какой темп обучения и т.д., и отсутствие принципиальной основы для ответа на эти вопросы. Вероятностная основа была реализована с использованием аппроксимаций МакКея [1992b] и с использованием Марковской цепи.\n",
    "Методы Монте-Карло (MCMC) по Нилу [1996]. Нил был аспирантом в той же лаборатории и в своей диссертации стремился показать, что, используя байесовский формализм, не обязательно иметь проблемы с \"переоснащением\", когда модели становятся большими, и нужно стремиться к пределу больших моделей. В то время как его собственная работа была посвящена сложным марковским цепным методам умозаключения в больших конечных сетях, он все же отметил, что некоторые из его сетей стали гауссовскими процессами в пределе бесконечного размера, и \"могут быть и более простые  как сделать вывод в этом случае\".\n",
    "\n",
    "It is perhaps interesting to mention a slightly wider historical perspective. The main reason why neural networks became popular was that they allowed the use of adaptive basis functions, as opposed to the well known linear models. The adaptive basis functions, or hidden units, could “learn” hidden features useful for the modelling problem at hand. However, this adaptivity came at the cost of a lot of practical problems. Later, with the advancement of the “kernel era”, it was realized that the limitation of fixed basis functions is not a big restriction if only one has enough of them, i.e. typically infinitely many, and one is careful to control problems of overfitting by using priors or regularization. The resulting models are much easier to handle than the adaptive basis function models, but have similar expressive power.\n",
    "\n",
    "Возможно, интересно упомянуть немного более широкую историческую перспективу. Основной причиной популярности нейросетей стало то, что они позволяли использовать адаптивные базисные функции, в отличие от хорошо известных линейных моделей. Адаптивные базовые функции, или скрытые единицы, могли \"изучать\" скрытые функции, полезные для решения конкретной задачи моделирования. Однако, эта адаптивность обошлась ценой множества практических проблем. Позднее, с развитием \"эпохи ядра\", стало понятно, что ограничение фиксированных базисных функций не является большим ограничением, если их достаточно только у одной, т.е. обычно их бесконечно много, и она тщательно контролирует проблемы переподготовки с помощью приоров или регуляризации. Полученные модели гораздо проще обрабатывать, чем адаптивные базовые модели функций, но при этом обладают аналогичной выразительной способностью.\n",
    "\n",
    "Thus, one could claim that (as far a machine learning is concerned) the adaptive basis functions were merely a decade-long digression, and we are now back to where we came from. This view is perhaps reasonable if we think of models for solving practical learning problems, although MacKay [2003, ch. 45], for example, raises concerns by asking “did we throw out the baby with the bath water?”, as the kernel view does not give us any hidden representations, telling us what the useful features are for solving a particular problem. As we will argue in the book, one answer may be to learn more sophisticated covariance functions, and the “hidden” properties of the problem are to be found here. An important area of future developments for GP models is the use of more expressive covariance functions.\n",
    "\n",
    "Таким образом, можно утверждать, что (с точки зрения машинного обучения) адаптивные базовые функции были всего лишь десятилетним отступлением, и теперь мы вернулись к тому, откуда пришли. Эта точка зрения, возможно, является разумной, если мы думаем о моделях решения практических задач обучения, хотя MacKay [2003, гл. 45], например, вызывает озабоченность, спрашивая: \"Выкинули ли мы ребенка с водой из ванны?\", так как точка зрения ядра не дает нам никаких скрытых представлений, говоря нам о том, какие полезные функции являются полезными для решения конкретной проблемы. Как мы будем утверждать в книге, одним из ответов может быть изучение более сложных ковариационных функций, а \"скрытые\" свойства проблемы можно найти здесь. Важной областью будущих разработок для GP моделей является использование более выразительных ковариационных функций.\n",
    "\n",
    "Supervised learning problems have been studied for more than a century in statistics, and a large body of well-established theory has been developed. More recently, with the advance of affordable, fast computation, the machine learning community has addressed increasingly large and complex problems.\n",
    "\n",
    "Проблемы обучения под наблюдением изучаются в статистике более ста лет, и был разработан большой объем устоявшейся теории. В последнее время, с продвижением доступных, быстрых вычислений, сообщество машинного обучения решает все более крупные и сложные проблемы.\n",
    "\n",
    "Much of the basic theory and many algorithms are shared between the statistics and machine learning community. The primary differences are perhaps the types of the problems attacked, and the goal of learning. At the risk of oversimplification, one could say that in statistics a prime focus is often in understanding the data and relationships in terms of models giving approximate summaries such as linear relations or independencies. In contrast, the goals in machine learning are primarily to make predictions as accurately as possible and to understand the behaviour of learning algorithms. These differing objectives have led to different developments in the two fields: for example, neural network algorithms have been used extensively as black-box function approximators in machine learning, but to many statisticians they are less than satisfactory, because of the difficulties in interpreting such models.\n",
    "\n",
    "Большая часть базовой теории и многие алгоритмы разделяются между статистикой и сообществом машинного обучения. Основные различия, возможно, заключаются в типах атакуемых проблем и цели обучения. Рискуя чрезмерным упрощением, можно сказать, что в статистике основное внимание часто уделяется пониманию данных и взаимосвязей с точки зрения моделей, дающих приблизительные обобщения, такие как линейные отношения или независимость. В отличие от этого, целью машинного обучения является, прежде всего, составление как можно более точных прогнозов и понимание поведения алгоритмов обучения. Эти различные цели привели к различным разработкам в этих двух областях: например, нейросетевые алгоритмы широко использовались в качестве аппроксиматоров функций \"черного ящика\" в машинном обучении, но для многих статистиков они являются менее чем удовлетворительными из-за трудностей интерпретации таких моделей.\n",
    "\n",
    "Gaussian process models in some sense bring together work in the two communities. As we will see, Gaussian processes are mathematically equivalent to many well known models, including Bayesian linear models, spline models, large neural networks (under suitable conditions), and are closely related to others, such as support vector machines. Under the Gaussian process viewpoint, the models may be easier to handle and interpret than their conventional counterparts, such as e.g. neural networks. In the statistics community Gaussian processes have also been discussed many times, although it would probably be excessive to claim that their use is widespread except for certain specific applications such as spatial models in meteorology and geology, and the analysis of computer experiments. A rich theory also exists for Gaussian process models in the time series analysis literature; some pointers to this literature are given in Appendix B.\n",
    "\n",
    "Гауссовские модели процессов в некотором смысле объединяют работу в двух сообществах. Как мы увидим, гауссовские процессы математически эквивалентны многим хорошо известным моделям, включая байесовские линейные модели, сплайновые модели, большие нейронные сети (в подходящих условиях), и тесно связаны с другими, например, с поддерживающими векторными машинами. С точки зрения гауссовских процессов, эти модели, возможно, легче обрабатывать и интерпретировать, чем их обычные аналоги, например, нейронные сети. В статистическом сообществе гауссовские процессы также многократно обсуждались, хотя, вероятно, было бы чрезмерным утверждать, что их использование широко распространено, за исключением некоторых специфических применений, таких как пространственные модели в метеорологии и геологии, а также анализ компьютерных экспериментов. Богатая теория для гауссовых моделей процессов также существует в литературе по анализу временных рядов; некоторые ссылки на эту литературу приведены в Приложении Б."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1. Introduction ##\n",
    "\n",
    "In this book we will be concerned with supervised learning, which is the problem of learning input-output mappings from empirical data (the training dataset). Depending on the characteristics of the output, this problem is known as either regression, for continuous outputs, or classification, when outputs are discrete.\n",
    "\n",
    "В этой книге мы поговорим об обучении под наблюдением, которое является проблемой изучения сопоставлений ввода-вывода из эмпирических данных (обучающего набора данных). В зависимости от характеристик выхода эта проблема известна как регрессия, для непрерывных выходов, или классификация, когда выходы дискретны.\n",
    "\n",
    "A well known example is the classification of images of handwritten digits. The training set consists of small digitized images, together with a classification from 0, . . . , 9, normally provided by a human. The goal is to learn a mapping from image to classification label, which can then be used on new, unseen images. Supervised learning is an attractive way to attempt to tackle this problem, since it is not easy to specify accurately the characteristics of, say, the handwritten digit 4.\n",
    "\n",
    "Хорошо известным примером является классификация изображений рукописных цифр. Учебный набор состоит из небольших оцифрованных изображений вместе с классификацией от 0, ... . 9, обычно предоставляемых человеком. Цель состоит в том, чтобы научиться составлять карты от изображения до классификационной метки, которые затем могут быть использованы на новых, невидимых изображениях. Обучение под наблюдением является привлекательным способом решения этой проблемы, поскольку не так просто точно указать характеристики, скажем, рукописной цифры 4.\n",
    "\n",
    "An example of a regression problem can be found in robotics, where we wish to learn the inverse dynamics of a robot arm. Here the task is to map from the state of the arm (given by the positions, velocities and accelerations of the joints) to the corresponding torques on the joints. Such a model can then be used to compute the torques needed to move the arm along a given trajectory. Another example would be in a chemical plant, where we might wish to predict the yield as a function of process parameters such as temperature, pressure, amount of catalyst etc.\n",
    "\n",
    "Пример проблемы регрессии можно найти в робототехнике, где мы хотим изучить обратную динамику руки робота. Здесь задача состоит в том, чтобы составить карту от состояния руки (заданного положением, скоростями и ускорениями суставов) до соответствующих крутящих моментов на суставах. Такая модель может быть использована для расчета крутящих моментов, необходимых для перемещения руки по заданной траектории. Другой пример - на химическом заводе, где мы могли бы предсказать выход как функцию от параметров процесса, таких как температура, давление, количество катализатора и т.д.\n",
    "\n",
    "Given this training data we wish to make predictions for new inputs x that we have not seen in the training set. Thus it is clear that the problem at hand is inductive; we need to move from the finite training data D to a function f that makes predictions for all possible input values. To do this we must make assumptions about the characteristics of the underlying function, as otherwise any function which is consistent with the training data would be equally valid. A wide variety of methods have been proposed to deal with the supervised learning problem; here we describe two common approaches. The first is to restrict the class of functions that we consider, for example by only considering linear functions of the input. The second approach is (speaking rather loosely) to give a prior probability to every possible function, where higher probabilities are given to functions that we consider to be more likely, for example because they are smoother than other functions.1 The first approach has an obvious problem in that we have to decide upon the richness of the class of functions considered; if we are using a model based on a certain class of functions (e.g. linear functions) and the target function is not well modelled by this class, then the predictions will be poor. One may be tempted to increase the flexibility of the class of functions, but this runs into the danger of overfitting, where we can obtain a good fit to the training data, but perform badly when making test predictions.\n",
    "\n",
    "Учитывая эти учебные данные, мы хотим сделать прогнозы для новых входных данных x, которые мы не видели в учебном наборе. Таким образом, становится ясно, что рассматриваемая проблема является индуктивной; мы должны перейти от конечных обучающих данных D к функции f, которая делает прогнозы для всех возможных входных значений. Для этого мы должны сделать предположения о свойствах базовой функции, так как в противном случае любая согласованная с обучающими данными функция была бы в равной степени действительна. Для решения проблемы обучения под наблюдением было предложено множество методов; здесь мы описываем два общих подхода. Первый заключается в ограничении класса функций, которые мы рассматриваем, например, рассматривая только линейные функции входа. Второй подход заключается (говоря довольно вольно) в том, чтобы дать предварительную вероятность каждой возможной функции, когда более высокие вероятности даются функциям, которые мы считаем более вероятными, например, потому, что они более гладкие, чем другие функции1 . Первый подход имеет очевидную проблему в том, что мы должны определиться с богатством рассматриваемого класса функций; если мы используем модель, основанную на определенном классе функций (например, линейных функциях), и целевая функция не очень хорошо смоделирована этим классом, то прогнозы будут плохими. Может возникнуть соблазн повысить гибкость класса функций, но при этом возникает опасность переподгонки, при которой можно получить хорошую подгонку к обучающим данным, но плохо выполнять предсказания при тестировании.\n",
    "\n",
    "The second approach appears to have a serious problem, in that surely there are an uncountably infinite set of possible functions, and how are we going to compute with this set in finite time? This is where the Gaussian process comes to our rescue. A Gaussian process is a generalization of the Gaussian probability distribution. Whereas a probability distribution describes random variables which are scalars or vectors (for multivariate distributions), a stochastic process governs the properties of functions. Leaving mathematical sophistication aside, one can loosely think of a function as a very long vector, each entry in the vector specifying the function value f(x) at a particular input x. It turns out, that although this idea is a little naive, it is surprisingly close what we need. Indeed, the question of how we deal computationally with these infinite dimensional objects has the most pleasant resolution imaginable: if you ask only for the properties of the function at a finite number of points, then inference in the Gaussian process will give you the same answer if you ignore the infinitely many other points, as if you would have taken them all into account! And these answers are consistent with answers to any other finite queries you may have. One of the main attractions of the Gaussian process framework is precisely that it unites a sophisticated and consistent view with computational tractability.\n",
    "\n",
    "Второй подход, похоже, имеет серьезную проблему, в том, что, несомненно, существует бесконечное множество возможных функций, и как мы будем вычислять с этим набором в конечное время? Именно здесь на помощь приходит гауссовский процесс. Гауссовский процесс - это обобщение гауссовского распределения вероятностей. В то время как распределение вероятности описывает случайные величины, которые являются скалярами или векторами (для многомерных распределений), стохастический процесс управляет свойствами функций. Оставив в стороне математическую сложность, можно свободно думать о функции как об очень длинном векторе, каждая запись в котором указывает значение функции f(x) на конкретном входе x. Оказывается, что хотя эта идея немного наивна, она на удивление близка к тому, что нам нужно. Действительно, вопрос о том, как мы имеем дело с этими бесконечными размерными объектами, имеет самое приятное разрешение, которое только можно себе представить: если задать только свойства функции на конечное число точек, то вывод в процессе Гаусса даст такой же ответ, если проигнорировать бесконечное множество других точек, как если бы Вы их все учли! И эти ответы согласуются с ответами на любые другие конечные запросы, которые у Вас могут быть. Одна из главных привлекательных особенностей фреймворка Гауссовых процессов как раз и заключается в том, что он объединяет сложный и последовательный взгляд с вычислительной траекторией.\n",
    "\n",
    "It should come as no surprise that these ideas have been around for some time, although they are perhaps not as well known as they might be. Indeed, many models that are commonly employed in both machine learning and statistics are in fact special cases of, or restricted kinds of Gaussian processes. In this volume, we aim to give a systematic and unified treatment of the area, showing connections to related models.\n",
    "\n",
    "Неудивительно, что эти идеи существуют уже некоторое время, хотя, возможно, они не так хорошо известны, как могли бы быть. Действительно, многие модели, которые обычно используются как в машинном обучении, так и в статистике, на самом деле являются особыми случаями или ограниченными видами гауссовских процессов. В этом томе мы стремимся дать систематическую и единую трактовку области, показывая связи с соответствующими моделями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 A Pictorial Introduction to Bayesian Modelling ###\n",
    "### 1.1 Пикториальное введение в байесовское моделирование ###\n",
    "\n",
    "In this section we give graphical illustrations of how the second (Bayesian) method works on some simple regression and classification examples.\n",
    "\n",
    "В этом разделе мы приводим графические иллюстрации того, как работает второй (байесовский) метод на некоторых простых примерах регрессии и классификации.\n",
    "\n",
    "We first consider a simple 1-d regression problem, mapping from an input x to an output f(x). In Figure 1.1(a) we show a number of sample functions drawn at random from the prior distribution over functions specified by a particular Gaussian process which favours smooth functions. This prior is taken to represent our prior beliefs over the kinds of functions we expect to observe, before seeing any data. In the absence of knowledge to the contrary we have assumed that the average value over the sample functions at each x is zero. Although the specific random functions drawn in Figure 1.1(a) do not have a mean of zero, the mean of f(x) values for any fixed x would become zero, independent of x as we kept on drawing more functions. At any value of x we can also characterize the variability of the sample functions by computing the variance at that point. The shaded region denotes twice the pointwise standard deviation; in this case we used a Gaussian process which specifies that the prior variance does not depend on x.\n",
    "\n",
    "Сначала мы рассмотрим простую задачу 1-d регрессии, отображение из входного x на выходной f(x). На рисунке 1.1(a) показан ряд выборочных функций, случайным образом выведенных из предыдущего распределения по функциям, заданным определенным гауссовским процессом, что благоприятствует гладким функциям. Этот предыдущий метод используется для того, чтобы представить наши предшествующие представления о том, какие функции мы ожидаем наблюдать, прежде чем увидеть какие-либо данные. В отсутствие знаний об обратном мы предположили, что среднее значение по функциям выборки при каждом x равно нулю. Хотя конкретные случайные функции, изображенные на рисунке 1.1(a), не имеют среднего значения нуля, среднее значение f(x) для любого фиксированного значения x стало бы нулевым, независимым от x, так как мы продолжали рисовать больше функций. При любом значении x можно также охарактеризовать изменчивость функций выборки, вычислив дисперсию в этот момент. Заштрихованная область обозначает вдвое больше точечного среднеквадратического отклонения; в данном случае мы использовали Гауссовский процесс, который указывает, что предыдущая дисперсия не зависит от x.\n",
    "\n",
    "Suppose that we are then given a dataset D = {(x1, y1), (x2, y2)} consist ing of two observations, and we wish now to only consider functions that pass though these two data points exactly. (It is also possible to give higher preference to functions that merely pass “close” to the datapoints.) This situation is illustrated in Figure 1.1(b). The dashed lines show sample functions which are consistent with D, and the solid line depicts the mean value of such functions. Notice how the uncertainty is reduced close to the observations. The combination of the prior and the data leads to the posterior distribution over functions.\n",
    "\n",
    "Предположим, что тогда нам дадут набор данных D = {(x1, y1), (x2, y2)}, состоящий из двух наблюдений, и теперь мы хотим рассмотреть только функции, которые точно проходят через эти две точки данных. (Можно также отдавать большее предпочтение функциям, которые просто проходят \"близко\" к точкам данных). Эта ситуация проиллюстрирована на рис. 1.1(b). На пунктирных линиях показаны примеры функций, которые соответствуют D, а на сплошной линии - среднее значение таких функций. Обратите внимание на то, как неопределенность уменьшается вблизи наблюдений. Сочетание предыдущего и данных приводит к последующему распределению по функциям.\n",
    "\n",
    "If more datapoints were added one would see the mean function adjust itself to pass through these points, and that the posterior uncertainty would reduce close to the observations. Notice, that since the Gaussian process is not a parametric model, we do not have to worry about whether it is possible for the model to fit the data (as would be the case if e.g. you tried a linear model on strongly non-linear data). Even when a lot of observations have been added, there may still be some flexibility left in the functions. One way to imagine the reduction of flexibility in the distribution of functions as the data arrives is to draw many random functions from the prior, and reject the ones which do not agree with the observations. While this is a perfectly valid way to do inference, it is impractical for most purposes—the exact analytical computations required to quantify these properties will be detailed in the next chapter.\n",
    "\n",
    "Если добавить больше точек данных, то можно увидеть, что средняя функция приспосабливается к прохождению через эти точки, и что апостериорная неопределенность уменьшится близко к наблюдениям. Обратите внимание, что поскольку Гауссовский процесс не является параметрической моделью, нам не нужно беспокоиться о возможности подгонки модели под данные (как это было бы, например, в случае, если бы вы попробовали линейную модель на сильно нелинейных данных). Даже если было добавлено много наблюдений, в функциях все равно может остаться некоторая гибкость. Один из способов представить себе снижение гибкости при распределении функций по мере поступления данных - взять много случайных функций из предыдущих и отвергнуть те, которые не согласны с наблюдениями. Хотя это и совершенно верный способ сделать вывод, он непрактичен для большинства целей - точные аналитические вычисления, необходимые для количественной оценки этих свойств, будут подробно описаны в следующей главе.\n",
    "\n",
    "The specification of the prior is important, because it fixes the properties of the functions considered for inference. Above we briefly touched on the mean and pointwise variance of the functions. However, other characteristics can also be specified and manipulated. Note that the functions in Figure 1.1(a) are smooth and stationary (informally, stationarity means that the functions look similar at all x locations). These are properties which are induced by the covariance function of the Gaussian process; many other covariance functions are possible. Suppose, that for a particular application, we think that the functions in Figure 1.1(a) vary too rapidly (i.e. that their characteristic length-scale is too short). Slower variation is achieved by simply adjusting parameters of the covariance function. The problem of learning in Gaussian processes is exactly the problem of finding suitable properties for the covariance function. Note, that this gives us a model of the data, and characteristics (such a  moothness, characteristic length-scale, etc.) which we can interpret.\n",
    "\n",
    "Спецификация предыдущей важна, так как она фиксирует свойства функций, рассматриваемых для умозаключения. Выше мы кратко коснулись средней и точечной дисперсии функций. Однако можно указывать и другие характеристики и манипулировать ими. Обратите внимание, что функции, показанные на рисунке 1.1(a), имеют следующие характеристики гладкий и стационарный (неофициально стационарность означает, что функции выглядят одинаково во всех x местах). Это свойства, которые индуцируются ковариационной функцией гауссовского процесса; возможны и многие другие ковариационные функции. Предположим, что для конкретного применения мы думаем, что функции, показанные на рисунке 1.1(a), слишком быстро меняются (т.е. их характерная шкала длины слишком коротка). Медленное изменение достигается простым регулированием параметров ковариационной функции. Проблема обучения в гауссовых процессах как раз и заключается в том, чтобы найти подходящие свойства для ковариационной функции. Отметим, что это дает нам модель данных и характеристик (такая спорность, характерная шкала длины и т.д.), которые мы можем интерпретировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2. Regression ##\n",
    "\n",
    "Supervised learning can be divided into regression and classification problems. Whereas the outputs for classification are discrete class labels, regression is concerned with the prediction of continuous quantities. For example, in a financial application, one may attempt to predict the price of a commodity as a function of interest rates, currency exchange rates, availability and demand. In this chapter we describe Gaussian process methods for regression problems; classification problems are discussed in chapter 3.\n",
    "\n",
    "Обучение под наблюдением можно разделить на регрессию и проблемы классификации. В то время как результаты для классификации представляют собой дискретные метки классов, регрессия касается прогнозирования непрерывных величин. Например, в финансовом приложении можно попытаться предсказать цену товара как функцию от процентных ставок, обменных курсов валют, наличия и спроса. В данной главе мы описываем методы Гауссовского процесса для решения проблем регрессии; проблемы классификации рассматриваются в главе 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Weight-space View ###\n",
    "### 2.1 Весовое пространство ###\n",
    "\n",
    "The simple linear regression model where the output is a linear combination of the inputs has been studied and used extensively. Its main virtues are simplicity of implementation and interpretability. Its main drawback is that it only allows a limited flexibility; if the relationship between input and output cannot reasonably be approximated by a linear function, the model will give poor predictions.\n",
    "\n",
    "Простая модель линейной регрессии, где выход является линейной комбинацией входов, была изучена и широко использована. Ее основными достоинствами являются простота реализации и интерпретируемость. Ее основным недостатком является то, что она допускает только ограниченную гибкость; если связь между входом и выходом не может быть обоснованно аппроксимирована линейной функцией, то модель будет давать плохие прогнозы.\n",
    "\n",
    "In this section we first discuss the Bayesian treatment of the linear model. We then make a simple enhancement to this class of models by projecting the inputs into a high-dimensional feature space and applying the linear model there. We show that in some feature spaces one can apply the “kernel trick” to\n",
    "carry out computations implicitly in the high dimensional space; this last step leads to computational savings when the dimensionality of the feature space is large compared to the number of data points.\n",
    "\n",
    "В этом разделе мы сначала обсудим байесовскую трактовку линейной модели. Затем мы сделаем простое дополнение к этому классу моделей, спроецировав входные данные в высокоразмерное пространство признаков и применив туда линейную модель. Мы покажем, что в некоторых пространствах признаков можно применить \"трюк кернела\", чтобы\n",
    "выполнять вычисления неявно в пространстве большой размерности; этот последний шаг приводит к экономии средств на вычислениях, когда размерность пространства признаков велика по сравнению с количеством точек данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Function-space View ###\n",
    "### 2.2 Функционально-пространственный просмотр ###\n",
    "\n",
    "An alternative and equivalent way of reaching identical results to the previous section is possible by considering inference directly in function space. We use a Gaussian process (GP) to describe a distribution over functions.\n",
    "\n",
    "Альтернативный и эквивалентный способ достижения результатов, идентичных предыдущему разделу, возможен путем рассмотрения вывода непосредственно в функциональном пространстве. Для описания распределения по функциям мы используем Гауссовский процесс (GP).\n",
    "\n",
    "***\n",
    "\n",
    "A Gaussian process is defined as a collection of random variables. Thus, the definition automatically implies a consistency requirement, which is also sometimes known as the marginalization property. This property simply means that if the GP e.g. specifies (y1, y2) \u0018 N(μ,\u0006), then it must also specify y1 \u0018 N(μ1,\u000611) where \u000611 is the relevant submatrix of \u0006, see eq. (A.6). In other words, examination of a larger set of variables does not change the distribution of the smaller set. Notice that the consistency requirement is automatically fulfilled if the covariance function specifies entries of the covariance\n",
    "matrix. The definition does not exclude Gaussian processes with finite index finite index set sets (which would be simply Gaussian distributions), but these are not particularly interesting for our purposes.\n",
    "\n",
    "Гауссовский процесс определяется как набор случайных величин. Таким образом, определение автоматически подразумевает требование непротиворечивости, которое также иногда называют свойством маргинализации. Это свойство просто означает, что если ВП, например, указывает (y1, y2) N(μ,), то оно также должно указывать y1 N(μ1,11), где 11 - соответствующая подматрица , см. экз. (A.6). Другими словами, изучение большего набора переменных не изменяет распределения меньшего набора. Обратите внимание, что требование консистенции выполняется автоматически, если ковариационная функция задает записи ковариаций\n",
    "matrix. Определение не исключает гауссовские процессы с наборами конечных индексов (которые были бы просто гауссовскими распределениями), но они не представляют особого интереса для наших целей.\n",
    "\n",
    "***\n",
    "\n",
    "In this chapter our running example of a covariance function will be the squared exponential (SE) covariance function; other covariance functions are discussed in chapter 4. The covariance function specifies the covariance between pairs of random variables. Note, that the covariance between the outputs is written as a function of the inputs. For this particular covariance function, we see that the covariance is almost unity between variables whose corresponding inputs are very close, and decreases as their distance in the input space increases.\n",
    "\n",
    "В этой главе нашим примером выполнения ковариационной функции будет квадратная экспоненциальная (SE) ковариационная функция; другие ковариационные функции рассматриваются в главе 4. Ковариационная функция определяет ковариацию между парами случайных величин. Обратите внимание, что ковариация между выходами записывается как функция входов. Для этой конкретной ковариационной функции мы видим, что коварианс - это почти единство между переменными, соответствующие входы которых очень близки, и уменьшается с увеличением их расстояния во входном пространстве.\n",
    "\n",
    "The specification of the covariance function implies a distribution over functions. To see this, we can draw samples from the distribution of functions evaluated at any number of points; in detail, we choose a number of input points, X\u0003 and write out the corresponding covariance matrix using eq. (2.16) elementwise. Then we generate a random Gaussian vector with this covariance matrix\n",
    "f\u0003 \u0018 N 􀀀 0,K(X\u0003,X\u0003) \u0001 , (2.17)\n",
    "and plot the generated values as a function of the inputs. Figure 2.2(a) shows three such samples. The generation of multivariate Gaussian samples is described in section A.2.\n",
    "\n",
    "Спецификация ковариационной функции подразумевает распределение по функциям. Для этого можно взять примеры из распределения функций, оцениваемых на произвольное количество точек, подробно выбрать количество входных точек, X и выписать соответствующую ковариационную матрицу с помощью функции \"Ковариационная функция\". (2.16) по элементам. Затем с помощью этой ковариационной матрицы генерируем случайный гауссовский вектор\n",
    "и построить график генерируемых значений в зависимости от входов. На рисунке 2.2(a) показаны три такие выборки. Генерация многомерных гауссовых выборок описана в разделе А.2.\n",
    "\n",
    "We also see that the functions seem to have a characteristic length-scale, which informally can be thought of as roughly the distance you have to move in input space before the function value can change significantly, see section 4.2.1. By replacing |xp−xq| by |xp−xq|/l in eq. (2.16) for some positive constant l we could change the characteristic length-scale of the process. Also, the overall variance of the random function can be controlled by a positive pre-factor before the exp in eq. (2.16). We will discuss more about how such factors affect the predictions in section 2.3, and say more about how to set such scale parameters in chapter 5.\n",
    "\n",
    "Мы также видим, что функции, кажется, имеют характерную шкалу длины, которая неформально может быть воспринята как примерное расстояние, которое необходимо преодолеть во входном пространстве, прежде чем значение функции может значительно измениться, см. раздел 4.2.1. Заменив |xp-xq| на |xp-xq|/l в каталоге (2.16) на некоторую положительную константу l, можно было бы изменить характерную шкалу длины процесса. Кроме того, общую дисперсию случайной функции можно контролировать положительным префактором перед exp в экз. (2.16). Подробнее о том, как такие факторы влияют на прогнозы, мы обсудим в разделе 2.3, а о том, как устанавливать такие масштабные параметры, расскажем в главе 5.\n",
    "\n",
    "***\n",
    "*p. 17*\n",
    "\n",
    "Let us examine the predictive distribution as given by equations 2.25 and 2.26. Note first that the mean prediction eq. (2.25) is a linear combination of observations y; this is sometimes referred to as a linear predictor . Another way to look at this equation is to see it as a linear combination of n kernel functions, each one centered on a training point, by writing where \u000b",
    " = (K + \u001b2nI)−1y. The fact that the mean prediction for f(x\u0003) can be written as eq. (2.27) despite the fact that the GP can be represented in terms of a (possibly infinite) number of basis functions is one manifestation of the representer theorem; see section 6.2 for more on this point. We can understand this result intuitively because although the GP defines a joint Gaussian distribution over all of the y variables, one for each point in the index set X, for making predictions at x\u0003 we only care about the (n+1)-dimensional distribution defined by the n training points and the test point. As a Gaussian distribution is marginalized by just taking the relevant block of the joint covariance matrix (see section A.2) it is clear that conditioning this (n+1)-dimensional distribution on the observations gives us the desired result. A graphical model representation of a GP is given in Figure 2.3.\n",
    "\n",
    "Рассмотрим прогностическое распределение, заданное уравнениями 2.25 и 2.26. Обратите внимание, что среднее предсказание eq. (2.25) является линейной комбинацией наблюдений y; его иногда называют линейным предиктором . Другой способ взглянуть на это уравнение - рассматривать его как линейную комбинацию n функций ядра, каждая из которых сосредоточена на обучающей точке, записывая где = (K + 2nI)-1y. Дело в том, что среднее предсказание для f(x) можно записать как eq. (2.27), несмотря на то, что ВП может быть представлено в виде (возможно, бесконечного) числа базисных функций, является одним из проявлений теоремы представления; подробнее об этой точке см. раздел 6.2. Этот результат мы можем понять интуитивно, поскольку хотя ВП и определяет совместное гауссово распределение по всем переменным y, по одному для каждой точки в наборе индексов X, для предсказания при x нас интересует только (n+1)-размерное распределение, определяемое n обучающими точками и контрольной точкой. Поскольку Гауссово распределение маргинализируется только принятием соответствующего блока совместной ковариационной матрицы (см. раздел А.2), то понятно, что обусловливание этого (n+1)-размерного распределения по наблюдениям дает нам желаемый результат. Графическое представление модели ВП приведено на рисунке 2.3.\n",
    "\n",
    "Note also that the variance in eq. (2.24) does not depend on the observed targets, but only on the inputs; this is a property of the Gaussian distribution. The variance is the difference between two terms: the first term K(X\u0003,X\u0003) is simply the prior covariance; from that is subtracted a (positive) term, representing the information the observations gives us about the function. We can very simply compute the predictive distribution of test targets y\u0003 by adding \u001b2nI to the variance in the expression for cov(f\u0003).\n",
    "\n",
    "Отметим также, что разница в эквиваленте (2.24) зависит не от наблюдаемых целей, а только от входных данных; это свойство гауссовского распределения. Дисперсия представляет собой разность между двумя членами: первый член K(X,X) является просто предшествующей ковариацией; из нее вычитается (положительный) член, представляющий информацию, которую наблюдения дают о функции. Мы можем очень просто вычислить предсказывающее распределение тестовых целей y, добавив 2nI к дисперсии в выражении для cov(f)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Varying the Hyperparameters ###\n",
    "### 2.3 Изменение гиперпараметров ###\n",
    "\n",
    "Typically the covariance functions that we use will have some free parameters. For example, the squared-exponential covariance function in one dimension has the following form.\n",
    "\n",
    "Обычно ковариационные функции, которые мы используем, имеют некоторые свободные параметры. Например, квадратно-экспоненциальная ковариационная функция в одном измерении имеет следующий вид.\n",
    "\n",
    "The covariance is denoted ky as it is for the noisy targets y rather than for the underlying function f. Observe that the length-scale l, the signal variance \u001b2f and the noise variance \u001b2n hyperparameters can be varied. In general we call the free parameters hyperparameters.\n",
    "\n",
    "Ковариансом называется ky, как и для шумных целей y, а не для базовой функции f. Обратите внимание, что гиперпараметры шкалы длины l, дисперсии сигнала 2f и дисперсии шума 2n могут варьироваться. В общем случае свободные параметры мы называем гиперпараметрами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4. Covariance Functions ##\n",
    "*p. 79*\n",
    "\n",
    "We have seen that a covariance function is the crucial ingredient in a Gaussian process predictor, as it encodes our assumptions about the function which we wish to learn. From a slightly different viewpoint it is clear that in supervised learning the notion of similarity between data points is crucial; it is a basic assumption that points with inputs x which are close are likely to have similar target values y, and thus training points that are near to a test point should be informative about the prediction at that point. Under the Gaussian process view it is the covariance function that defines nearness or similarity.\n",
    "\n",
    "Мы видели, что ковариационная функция является важнейшим ингредиентом гауссовского предсказателя процессов, поскольку она кодирует наши предположения о функции, которую мы хотим изучить. С несколько иной точки зрения очевидно, что при обучении под наблюдением решающее значение имеет понятие сходства между точками данных; это основное предположение, что точки с близкими входами x, скорее всего, будут иметь сходные целевые значения y, и, таким образом, обучающие точки, находящиеся вблизи контрольной точки, должны быть информативными в отношении предсказания в этой точке. С точки зрения Гауссова процесса, именно ковариационная функция определяет близость или сходство.\n",
    "\n",
    "An arbitrary function of input pairs x and x0 will not, in general, be a valid valid covariance covariance function.1 The purpose of this chapter is to give examples of some functions commonly-used covariance functions and to examine their properties. Section 4.1 defines a number of basic terms relating to covariance functions. Section 4.2 gives examples of stationary, dot-product, and other non-stationary covariance functions, and also gives some ways to make new ones from old. Section 4.3 introduces the important topic of eigenfunction analysis of covariance functions, and states Mercer’s theorem which allows us to express the covariance function (under certain conditions) in terms of its eigenfunctions and eigenvalues. The covariance functions given in section 4.2 are valid when the input domain X is a subset of RD. In section 4.4 we describe ways to define covariance functions when the input domain is over structured objects such as strings and trees.\n",
    "\n",
    "Произвольная функция входных пар x и x0, в общем случае, не является действительной ковариационной функцией.1 Целью данной главы является приведение примеров некоторых часто используемых ковариационных функций и исследование их свойств. Раздел 4.1 определяет ряд основных терминов, относящихся к ковариационным функциям. В разделе 4.2 приведены примеры стационарных, точечных и других нестационарных ковариационных функций, а также даны некоторые способы создания новых из старых. Раздел 4.3 вводится важная тема анализа собственных функций ковариационных функций, и излагается теорема Мерсера, позволяющая выразить ковариационную функцию (при определенных условиях) с точки зрения собственных функций и собственных значений. Ковариационные функции, приведенные в разделе 4.2, действительны, когда входной домен X является подмножеством RD. В разделе 4.4 мы описываем способы определения ковариационных функций, когда входной домен находится над такими структурированными объектами, как строки и деревья."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5. Model Selection and Adaptation of Hyperparameters ##\n",
    "*p. 105*\n",
    "\n",
    "In chapters 2 and 3 we have seen how to do regression and classification using a Gaussian process with a given fixed covariance function. However, in many practical applications, it may not be easy to specify all aspects of the covariance function with confidence. While some properties such as stationarity of the covariance function may be easy to determine from the context, we typically have only rather vague information about other properties, such as the value of free (hyper-) parameters, e.g. length-scales. In chapter 4 several examples of covariance functions were presented, many of which have large numbers of parameters. In addition, the exact form and possible free parameters of the likelihood function may also not be known in advance. Thus in order to turn Gaussian processes into powerful practical tools it is essential to develop methods that address the model selection problem. We interpret the model selection problem rather broadly, to include all aspects of the model including the discrete choice of the functional form for the covariance function as well as values for any hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В главах 2 и 3 мы видели, как делать регрессию и классификацию, используя гауссовский процесс с заданной фиксированной ковариационной функцией. Однако во многих практических приложениях может быть нелегко с уверенностью указать все аспекты ковариационной функции. Хотя некоторые свойства, такие как стационарность ковариационной функции, могут быть легко определены из контекста, мы обычно имеют лишь достаточно расплывчатую информацию о других свойствах, например, о значении свободных (гипер-) параметров, например, о длинах. В главе 4 было представлено несколько примеров ковариационных функций, многие из которых имеют большое количество параметров. Кроме того, точная форма и возможные свободные параметры вероятностной функции также могут быть неизвестны заранее. Таким образом, для того, чтобы превратить гауссовские процессы в мощный практический инструмент, необходимо разработать методы, решающие проблему выбора модели. Мы достаточно широко интерпретируем проблему выбора модели, чтобы включить в нее все аспекты модели, в том числе и дискретные выбор функциональной формы для ковариационной функции, а также значения для любых гиперпараметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
