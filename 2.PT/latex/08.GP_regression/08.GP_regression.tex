\documentclass[11pt,a4paper]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)

    \usepackage{iftex}
    \ifPDFTeX
      \usepackage[T2A]{fontenc}
      \usepackage{mathpazo}
      \usepackage[russian,english]{babel}
    \else
      \usepackage{fontspec}
      \usepackage{polyglossia}
      \setmainlanguage[babelshorthands=true]{russian}    % Язык по-умолчанию русский с поддержкой приятных команд пакета babel
      \setotherlanguage{english}                         % Дополнительный язык = английский (в американской вариации по-умолчанию)

      \defaultfontfeatures{Ligatures=TeX}
      \setmainfont[BoldFont={STIX Two Text SemiBold}]%
      {STIX Two Text}                                    % Шрифт с засечками
      \newfontfamily\cyrillicfont[BoldFont={STIX Two Text SemiBold}]%
      {STIX Two Text}                                    % Шрифт с засечками для кириллицы
      \setsansfont{Fira Sans}                            % Шрифт без засечек
      \newfontfamily\cyrillicfontsf{Fira Sans}           % Шрифт без засечек для кириллицы
      \setmonofont[Scale=0.87,BoldFont={Fira Mono Medium},ItalicFont=[FiraMono-Oblique]]%
      {Fira Mono}%                                       % Моноширинный шрифт
      \newfontfamily\cyrillicfonttt[Scale = 0.87,BoldFont={Fira Mono Medium},ItalicFont=[FiraMono-Oblique]]%
      {Fira Mono}                                        % Моноширинный шрифт для кириллицы

      %%% Математические пакеты %%%
      \usepackage{amsthm,amsmath,amscd}   % Математические дополнения от AMS
      \usepackage{amsfonts,amssymb}       % Математические дополнения от AMS
      \usepackage{mathtools}              % Добавляет окружение multlined
      \usepackage{unicode-math}           % Для шрифта STIX Two Math
      \setmathfont{STIX Two Math}         % Математический шрифт
    \fi
    \renewcommand{\linethickness}{0.1ex}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}



    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{
      {\Large Лекция 8} \\
      Регрессия на основе гауссовских процессов
    }
    \date{13 апреля 2022\,г.}



% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}



\begin{document}

 \maketitle
\thispagestyle{empty}
\tableofcontents

\let\thefootnote\relax\footnote{
  \textit{День 13 апреля в истории:
    \begin{itemize}[topsep=2pt,itemsep=1pt]
      \item 1111 г. --- Генрих V коронован императором Священной Римской империи;
      \item 1204 г. --- захват и разграбление Константинополя участниками Четвёртого Крестового похода;
      \item 1904 г. --- у Порт-Артура при взрыве броненосца <<Петропавловск>> погибли художник Василий Верещагин и вице-адмирал Степан Макаров;
      \item 1967 г. --- Первое выступление The Rolling Stones за <<железным занавесом>> в Варшаве.
    \end{itemize}
  }
}
\newpage


\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Imports}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{42}\PY{p}{)}

\PY{k+kn}{import} \PY{n+nn}{sys}
\PY{n}{sys}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./modules}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k+kn}{import} \PY{n+nn}{GP\PYZus{}kernels}
\PY{k+kn}{from} \PY{n+nn}{GP\PYZus{}utils} \PY{k+kn}{import} \PY{n}{plot\PYZus{}GP}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Styles, fonts}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}
\PY{n}{matplotlib}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{font.size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{12}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{gridspec} \PY{k}{as} \PY{n+nn}{gridspec}
\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k+kn}{import} \PY{n}{cm} \PY{c+c1}{\PYZsh{} Colormaps}

\PY{k+kn}{import} \PY{n+nn}{seaborn}
\PY{n}{seaborn}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{whitegrid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} \PYZpc{}config InlineBackend.figure\PYZus{}formats = [\PYZsq{}pdf\PYZsq{}]}
\PY{c+c1}{\PYZsh{} \PYZpc{}config Completer.use\PYZus{}jedi = False}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

    \hypertarget{ux43dux435ux43fux430ux440ux430ux43cux435ux442ux440ux438ux447ux435ux441ux43aux430ux44f-ux440ux435ux433ux440ux435ux441ux441ux438ux44f}{%
\section{Непараметрическая
регрессия}\label{ux43dux435ux43fux430ux440ux430ux43cux435ux442ux440ux438ux447ux435ux441ux43aux430ux44f-ux440ux435ux433ux440ux435ux441ux441ux438ux44f}}

    В методах обучения с учителем мы часто используем \emph{параметрические
модели} \(p(\mathbf{y} \lvert \mathbf{X},\boldsymbol\theta)\) для
аппроксимации данных и нахождения оптимальных значений параметра
\(\boldsymbol\theta\) с помощью метода максимального правдоподобия или
метода оценки апостериорного максимума. С ростом сложности данных для
построения хорошей аппроксимации обычно требуются модели с большим
количеством параметров. Методы, использующие модели с фиксированным
количеством параметров, называются параметрическими методами.

    Другим подходом является использование \emph{непараметрических методов},
где количество параметров зависит от размера набора данных. Например, в
ядерной регрессии Надарая--Ватсона каждому наблюдаемому отклику \(y_i\)
присваивается вес \(w_i\), а для предсказания целевого значения в новой
точке \(\mathbf{x}\) вычисляется средневзвешенное значение:
\[
  f(x) = \frac{\sum_{i=1}^{N} k(x, x_i) y_i} {\sum_{i=1}^{N} k(x, x_i)}.
\]

Эта формула интуитивно очевидна: значение \(f(x)\) есть взвешенное
среднее \(y_i\) по объектам \(x_i\), причём чем ближе объекты к \(x\),
тем больше их вес. Веса зависят от \(x\) и наблюдаемых \(x_i\), а
функция зависимости \(k(x, x_i)\) называется \emph{ядром}.


    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Data}
\PY{n}{xlim} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.}\PY{p}{,} \PY{l+m+mf}{10.}\PY{p}{]}
\PY{n}{N\PYZus{}test} \PY{o}{=} \PY{l+m+mi}{101}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{*}\PY{n}{xlim}\PY{p}{,} \PY{n}{N\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{2.}\PY{p}{,}  \PY{l+m+mf}{6.}\PY{p}{,}  \PY{l+m+mf}{7.}\PY{p}{,} \PY{l+m+mf}{8.}\PY{p}{,}  \PY{l+m+mf}{4.}\PY{p}{,} \PY{l+m+mf}{3.} \PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{Y\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{1.}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{1.}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{NW\PYZus{}predictor}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,} \PY{n}{kernel\PYZus{}fun}\PY{p}{,} \PY{n}{kernel\PYZus{}args}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}\PYZsq{}\PYZsq{}\PYZsq{}}
    \PY{n}{K} \PY{o}{=} \PY{n}{kernel\PYZus{}fun}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{kernel\PYZus{}args}\PY{p}{)}
    \PY{n}{Weights} \PY{o}{=} \PY{n}{K} \PY{o}{/} \PY{n}{K}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{k}{return} \PY{n}{Weights} \PY{o}{@} \PY{n}{Y\PYZus{}train}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{start}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{n}{x\PYZus{}train\PYZus{}i} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{n}{i}\PY{p}{]}
    \PY{n}{y\PYZus{}train\PYZus{}i} \PY{o}{=} \PY{n}{Y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{n}{i}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} Compute mean and covariance of the posterior predictive distribution}
    \PY{n}{kernel\PYZus{}fun} \PY{o}{=} \PY{n}{GP\PYZus{}kernels}\PY{o}{.}\PY{n}{gauss}
    \PY{n}{kernel\PYZus{}args} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+m+mf}{1.}\PY{p}{\PYZcb{}}
    \PY{n}{Y\PYZus{}test} \PY{o}{=} \PY{n}{NW\PYZus{}predictor}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{x\PYZus{}train\PYZus{}i}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}i}\PY{p}{,} \PY{n}{kernel\PYZus{}fun}\PY{p}{,} \PY{n}{kernel\PYZus{}args}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Draw samples from the prior}
    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}test}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}train\PYZus{}i}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}i}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kx}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_12_0.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_12_1.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_12_2.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_12_3.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_12_4.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_12_5.pdf}
    \end{center}
%    { \hspace*{\fill} \\}

Примером является метод KNN (k-nearest neighbors), где \(k\) самых
близких наблюдений имеют вес \(1/k\), а все остальные --- \(0\). Обычно,
для вычисления прогноза непараметрические методы должны обработать все
обучающие данные и поэтому работают медленнее, чем параметрические
методы. Зато само обучение обычно происходит быстро, так как
непараметрическим моделям нужно только запомнить обучающие данные.

Ещё одним примером непараметрического метода являются гауссовские
процессы. Поскольку гауссовские процессы моделируют распределение
функций, мы можем использовать их для построения регрессионных моделей.

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

    \hypertarget{ux440ux435ux433ux440ux435ux441ux441ux438ux44f-ux43dux430-ux43eux441ux43dux43eux432ux435-ux433ux430ux443ux441ux441ux43eux432ux441ux43aux438ux445-ux43fux440ux43eux446ux435ux441ux441ux43eux432}{%
\section{Регрессия на основе гауссовских
процессов}\label{ux440ux435ux433ux440ux435ux441ux441ux438ux44f-ux43dux430-ux43eux441ux43dux43eux432ux435-ux433ux430ux443ux441ux441ux43eux432ux441ux43aux438ux445-ux43fux440ux43eux446ux435ux441ux441ux43eux432}}

    В регрессии на основе гауссовских процессов предполагается, что заданная
выборка значений целевой функции является реализацией гауссовского
процесса и ковариационная функция процесса зависит только от взаимного
расположения точек в пространстве. Апостериорное среднее гауссовского
процесса в новой точке используется для прогноза целевой функции в
данной точке, а апостериорная дисперсия используется в качестве оценки
неопределённости этого прогноза.

Обычно предполагают, что ковариационная функция гауссовского процесса
лежит в параметрическом семействе. Тогда задание регрессионной модели
эквивалентно выбору параметров ковариационной функции. Для оценки
параметров используют метод максимального правдоподобия и байесовские
методы.

    \hypertarget{ux444ux43eux440ux43cux443ux43bux44b}{%
\subsection{Формулы}\label{ux444ux43eux440ux43cux443ux43bux44b}}

Вспомним выражения для условного (апостериорного) распределения Гаусса.

Итак, мы хотим сделать прогноз \(y_2 = f(X_2)\) для \(n_2\) новых
объектов, основываясь на \(n_1\) ранее наблюдаемых объектах
\((X_1,y_1)\). Это можно сделать с помощью апостериорного распределения
\(p(y_2 \mid y_1, X_1, X_2)\). Предполагая, что \(y_1\) и \(y_2\) имеют
совместное нормальное распределение, мы можем написать:
\[
\left[\begin{array}{c} \mathbf{y}_{1} \\ \mathbf{y}_{2} \end{array}\right]
\sim
\mathcal{N} \left(
\left[\begin{array}{c} \mu_{1} \\ \mu_{2} \end{array}\right],
\left[ \begin{array}{cc}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{array} \right]
\right).
\]

Здесь: \[
\begin{aligned}
    \mu_{1}     &= m(X_1)     \quad && (n_1 \times 1),   \\
    \mu_{2}     &= m(X_2)     \quad && (n_2 \times 1),   \\
    \Sigma_{11} &= k(X_1,X_1) \quad && (n_1 \times n_1), \\
    \Sigma_{22} &= k(X_2,X_2) \quad && (n_2 \times n_2), \\
    \Sigma_{12} &= k(X_1,X_2) \quad && (n_1 \times n_2), \\
    \Sigma_{21} &= k(X_2,X_1) \quad && (n_2 \times n_1).
\end{aligned}
\]

    Для простоты положим \(\mu_{1} = 0\) и \(\mu_{2} = 0\). Тогда для
апостериорного распределения получим \[
  p(y_2 \mid y_1) = \mathcal{N}(\mu_{2|1}, \Sigma_{2|1}),
\] где \[
\begin{split}
    \mu_{2|1}    &= \Sigma_{21} \Sigma_{11}^{-1} y_1, \\
    \Sigma_{2|1} &= \Sigma_{22} - \Sigma_{21} \Sigma_{11}^{-1}\Sigma_{12}.
\end{split}
\]

Теперь мы можем предсказать \(y_2\), используя апостериорное среднее
\(\mu_{2|1}\) в качестве прогноза. Обратим внимание, что кривая
регрессии \(\mu_{2|1}\) является суммой с весами наблюдаемых значений
\(y_1\), где веса вычисляются с помощью ковариационной функции \(k\).
Оценить точность прогноза можно с помощью значений дисперсии
\(\sigma_2^2\), стоящих на диагонали апостериорной ковариационной
матрицы \(\Sigma_{2|1}\).

    \hypertarget{ux440ux435ux430ux43bux438ux437ux430ux446ux438ux44f}{%
\subsection{Реализация}\label{ux440ux435ux430ux43bux438ux437ux430ux446ux438ux44f}}

Во-первых, заметим, что выражения для апостериорного математического
ожидания и апостериорной ковариационной матрицы можно записать в более
адаптированном к вычислениям виде: \[
\begin{split}
    \mu_{2|1}    &\;= (\Sigma_{11}^{-1}\Sigma_{12})^\top  y_1, \\
    \Sigma_{2|1} &\;= \Sigma_{22} - (\Sigma_{11}^{-1}\Sigma_{12})^\top  \Sigma_{12}.
\end{split}
\]

    Мы будем использовать процессы с квадратичным экспоненциальным ядром: \[
  k(x, x') = \sigma_k^2 \exp{ \left( -\frac{\lVert x - x' \rVert^2}{2\ell^2}  \right) }.
\]

Ниже приводится функция \texttt{GP\_predictor()}, которая делают всю
работу. Матрица \(\Sigma_{11}^{-1}\Sigma_{12}\) вычисляется с помощью
функции \texttt{numpy.linalg.solve()}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{GP\PYZus{}predictor}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,}
                 \PY{n}{kernel\PYZus{}fun}\PY{p}{,} \PY{n}{kernel\PYZus{}args}\PY{p}{,} \PY{n}{sigma\PYZus{}n}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{    Computes the suffiсient statistics of the GP posterior predictive distribution }
\PY{l+s+sd}{    from m training data X\PYZus{}train and Y\PYZus{}train and n new inputs X\PYZus{}test.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        X\PYZus{}test: New input locations (n x d)}
\PY{l+s+sd}{        X\PYZus{}train: Training locations (m x d)}
\PY{l+s+sd}{        Y\PYZus{}train: Training targets (m x 1)}
\PY{l+s+sd}{        kernel\PYZus{}fun: Kernel length parameter}
\PY{l+s+sd}{        kernel\PYZus{}args: Kernel vertical variation parameter}
\PY{l+s+sd}{        sigma\PYZus{}n: Noise parameter}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        Posterior mean vector (n x d) and covariance matrix (n x n)}
\PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
    
    \PY{n}{K\PYZus{}11} \PY{o}{=} \PY{n}{kernel\PYZus{}fun}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{kernel\PYZus{}args}\PY{p}{)} \PYZbs{}
         \PY{o}{+} \PY{n}{sigma\PYZus{}n}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
    \PY{n}{K\PYZus{}12} \PY{o}{=} \PY{n}{kernel\PYZus{}fun}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{kernel\PYZus{}args}\PY{p}{)}
    \PY{n}{K\PYZus{}solved} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{solve}\PY{p}{(}\PY{n}{K\PYZus{}11}\PY{p}{,} \PY{n}{K\PYZus{}12}\PY{p}{)}\PY{o}{.}\PY{n}{T}
    \PY{n}{K\PYZus{}22} \PY{o}{=} \PY{n}{kernel\PYZus{}fun}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,}  \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{kernel\PYZus{}args}\PY{p}{)}
    
    \PY{n}{mu}  \PY{o}{=} \PY{n}{K\PYZus{}solved} \PY{o}{@} \PY{n}{Y\PYZus{}train}
    \PY{n}{cov} \PY{o}{=} \PY{n}{K\PYZus{}22} \PY{o}{\PYZhy{}} \PY{n}{K\PYZus{}solved} \PY{o}{@} \PY{n}{K\PYZus{}12}
    
    \PY{k}{return} \PY{n}{mu}\PY{p}{,} \PY{n}{cov}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

    \hypertarget{ux442ux435ux441ux442ux438ux440ux43eux432ux430ux43dux438ux435}{%
\section{Тестирование}\label{ux442ux435ux441ux442ux438ux440ux43eux432ux430ux43dux438ux435}}

    \hypertarget{ux43eux431ux443ux447ux430ux44eux449ux430ux44f-ux432ux44bux431ux43eux440ux43aux430-ux431ux435ux437-ux448ux443ux43cux430-ux438ux43dux442ux435ux440ux43fux43eux43bux44fux446ux438ux44f}{%
\subsection{Обучающая выборка без шума
(интерполяция)}\label{ux43eux431ux443ux447ux430ux44eux449ux430ux44f-ux432ux44bux431ux43eux440ux43aux430-ux431ux435ux437-ux448ux443ux43cux430-ux438ux43dux442ux435ux440ux43fux43eux43bux44fux446ux438ux44f}}

Приведенный ниже код вычисляет апостериорное распределение при заданной
обучающей выборке. Результаты представлены ниже. Чёрным цветом показана
линия регрессии --- функция апостериорного среднего \(\mu_{2|1}(x)\).
Разноцветные линии --- примеры траекторий процесса. На рисунках видно,
что точки обучающей выборки как будто захватывают траектории процесса в
узкие пучки.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{N\PYZus{}gp\PYZus{}post} \PY{o}{=} \PY{l+m+mi}{50}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{n}{x\PYZus{}train\PYZus{}i} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{n}{i}\PY{p}{]}
    \PY{n}{y\PYZus{}train\PYZus{}i} \PY{o}{=} \PY{n}{Y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{n}{i}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} Compute mean and covariance of the posterior predictive distribution}
    \PY{n}{kernel\PYZus{}fun} \PY{o}{=} \PY{n}{GP\PYZus{}kernels}\PY{o}{.}\PY{n}{gauss}
    \PY{n}{kernel\PYZus{}args} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+m+mf}{1.}\PY{p}{\PYZcb{}}
    \PY{n}{mu}\PY{p}{,} \PY{n}{cov} \PY{o}{=} \PY{n}{GP\PYZus{}predictor}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{x\PYZus{}train\PYZus{}i}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}i}\PY{p}{,}
                           \PY{n}{kernel\PYZus{}fun}\PY{p}{,} \PY{n}{kernel\PYZus{}args}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Generate samples from the prior}
    \PY{n}{gp\PYZus{}post} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{multivariate\PYZus{}normal}\PY{p}{(}\PY{n}{mu}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{cov}\PY{p}{,} \PY{n}{N\PYZus{}gp\PYZus{}post}\PY{p}{)}\PY{o}{.}\PY{n}{T}
    
    \PY{c+c1}{\PYZsh{} Draw samples from the prior}
    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
    \PY{n}{plot\PYZus{}GP}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{cov}\PY{p}{,} \PY{n}{x\PYZus{}train\PYZus{}i}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}i}\PY{p}{,} \PY{n}{gp\PYZus{}post}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_24_0.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_24_1.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_24_2.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_24_3.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_24_4.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_24_5.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.65\linewidth}{0.65\paperheight}}{output_24_6.pdf}
    \end{center}
%    { \hspace*{\fill} \\}


Получим выражения для первых двух кривых в явном виде:
    
\begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item \(f(x | x_1, y_1) = y_1 \cdot e^{-\frac{1}{2}(x-x_1)^2} = e^{-\frac{1}{2}(x-2)^2}\)
  \item учитывая, что
    \(k_{12} = e^{-\frac{1}{2}(x_2 - x_1)^2} = e^{-\frac{1}{2}(6 - 2)^2} = 3.4 \cdot 10^{-4} \approx 0\),\\
    получим
    \(f(x | x_1, y_1, x_2, y_2) \approx y_1 \cdot e^{-\frac{1}{2}(x-x_1)^2} + y_2 \cdot e^{-\frac{1}{2}(x-x_2)^2} = e^{-\frac{1}{2}(x-2)^2} - e^{-\frac{1}{2}(x-6)^2}\)
\end{enumerate}


В завершение приведём последний рисунок ещё раз, но уже без траекторий процесса.
Серым цветом показана оценка неопределённости прогноза регрессии: два
среднеквадратичных отклонения \(\sigma_{2|1}(x)\) в обе стороны.
Значения \(\sigma_{2|1}(x)\) вычисляются как корни из диагональных
элементов апостериорной ковариационной матрицы \(\Sigma_{2|1}\).
Отметим, что неопределённость прогноза равна нулю в точках обучающей
выборки и возрастает по мере удаления от этих точек.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Compute mean and covariance of the posterior predictive distribution}
\PY{n}{kernel\PYZus{}fun} \PY{o}{=} \PY{n}{GP\PYZus{}kernels}\PY{o}{.}\PY{n}{gauss}
\PY{n}{kernel\PYZus{}args} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+m+mf}{1.}\PY{p}{\PYZcb{}}
\PY{n}{mu}\PY{p}{,} \PY{n}{cov} \PY{o}{=} \PY{n}{GP\PYZus{}predictor}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,}
                       \PY{n}{kernel\PYZus{}fun}\PY{p}{,} \PY{n}{kernel\PYZus{}args}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Generate samples from the prior}
\PY{n}{gp\PYZus{}post} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{multivariate\PYZus{}normal}\PY{p}{(}\PY{n}{mu}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{cov}\PY{p}{,} \PY{n}{N\PYZus{}gp\PYZus{}post}\PY{p}{)}\PY{o}{.}\PY{n}{T}
    
\PY{c+c1}{\PYZsh{} Draw samples from the prior}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}GP}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{cov}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,} \PY{n}{draw\PYZus{}ci}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.8\linewidth}{0.8\paperheight}}{output_26_0.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    

    \hypertarget{ux43eux431ux443ux447ux430ux44eux449ux430ux44f-ux432ux44bux431ux43eux440ux43aux430-ux441-ux448ux443ux43cux43eux43c-ux430ux43fux43fux440ux43eux43aux441ux438ux43cux430ux446ux438ux44f}{%
\subsection{Обучающая выборка с шумом
(аппроксимация)}\label{ux43eux431ux443ux447ux430ux44eux449ux430ux44f-ux432ux44bux431ux43eux440ux43aux430-ux441-ux448ux443ux43cux43eux43c-ux430ux43fux43fux440ux43eux43aux441ux438ux43cux430ux446ux438ux44f}}

    Построенная регрессия предполагает, что обучающие данные
\(f(X_1) = \mathbf{y}_1\) абсолютно точны. Это можно заметить на
приведенном выше графике, так как в точках обучающей выборки
\((X_1,\mathbf{y}_1)\) апостериорная дисперсия становится равной нулю.

А что, если мы не на 100 \% уверены в обучающих данных? То есть
значения, через которые мы проводим процесс, измерены с какой-то
точностью. В таком случае можно делать прогноз по шумным наблюдениям
\(f(X_1) = \mathbf{y}_1 + \varepsilon\), смоделировав шум с помощью
нормальной случайной величиной нулевым математическим ожиданием и дисперсией
\(\sigma_n\): \(\varepsilon \sim \mathcal{N}(0, \sigma_n^2)\).
Это можно сделать, добавив диагональную матрицу к ковариационному ядру
наших наблюдений (вспоминаем гребневую регрессию):
\[
  \Sigma_{11} = k(X_1,X_1) + \sigma_n^2 I
\]
Отметим, что шум изменяет только диагональные элементы ядра (белый шум
некоррелирован).

Приведённый ниже код строит регрессию не тех же обучающих данных, но в
этот раз с добавлением шума.
На рисунках видно, что дисперсия \(\sigma_{2|1}^2\) в точках обучающей
выборки больше не равна 0, а линия регрессии и траектории процесса
больше не обязаны проходить через эти точки.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{N\PYZus{}gp\PYZus{}post} \PY{o}{=} \PY{l+m+mi}{50}
\PY{n}{sigma\PYZus{}n} \PY{o}{=} \PY{l+m+mf}{0.2}
    
\PY{c+c1}{\PYZsh{} Compute mean and covariance of the posterior predictive distribution}
\PY{n}{kernel\PYZus{}fun} \PY{o}{=} \PY{n}{GP\PYZus{}kernels}\PY{o}{.}\PY{n}{gauss}
\PY{n}{kernel\PYZus{}args} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+m+mf}{1.}\PY{p}{\PYZcb{}}
\PY{n}{mu}\PY{p}{,} \PY{n}{cov} \PY{o}{=} \PY{n}{GP\PYZus{}predictor}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,}
                       \PY{n}{kernel\PYZus{}fun}\PY{p}{,} \PY{n}{kernel\PYZus{}args}\PY{p}{,} \PY{n}{sigma\PYZus{}n}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Generate samples from the prior}
\PY{n}{gp\PYZus{}post} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{multivariate\PYZus{}normal}\PY{p}{(}\PY{n}{mu}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{cov}\PY{p}{,} \PY{n}{N\PYZus{}gp\PYZus{}post}\PY{p}{)}\PY{o}{.}\PY{n}{T}

\PY{c+c1}{\PYZsh{} Draw samples from the prior}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}GP}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{cov}\PY{p}{,} \PY{n}{x\PYZus{}train\PYZus{}i}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}i}\PY{p}{,} \PY{n}{gp\PYZus{}post}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.8\linewidth}{0.8\paperheight}}{output_31_0.pdf}
    \end{center}
%    { \hspace*{\fill} \\}
    
    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

    \hypertarget{ux438ux441ux442ux43eux447ux43dux438ux43aux438}{%
\section{Источники}\label{ux438ux441ux442ux43eux447ux43dux438ux43aux438}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Воронцов К.В.}
  \href{http://www.machinelearning.ru/wiki/images/6/6d/Voron-ML-1.pdf}{Математические
  методы обучения по прецедентам (теория обучения машин)}. --- 141 c.
\item
  \emph{Roelants P.}
  \href{https://peterroelants.github.io/posts/gaussian-process-tutorial/}{Understanding
  Gaussian processes}.
\item
  \emph{Krasser M.}
  \href{http://krasserm.github.io/2018/03/19/gaussian-processes/}{Gaussian
  processes}.
\item
  \emph{Зайцев А.А.} Методы построения регрессионных моделей разнородных
  источников данных для индустриальной инженерии // Диссертация на
  соискание учёной степени кандидата физико-математических наук. ИППИ
  РАН, 2017. 148 с.
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Versions used}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Python: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{.}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{.}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{o}{*}\PY{n}{sys}\PY{o}{.}\PY{n}{version\PYZus{}info}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{numpy: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{matplotlib: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{matplotlib}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seaborn: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{seaborn}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Python: 3.7.11
numpy: 1.20.3
matplotlib: 3.5.1
seaborn: 0.11.2
    \end{Verbatim}



    % Add a bibliography block to the postdoc
    
    
    
\end{document}
